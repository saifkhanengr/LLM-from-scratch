{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us68upxgCv-F"
      },
      "source": [
        "# Large Language Model (LLM) from Scratch\n",
        "## DeepSeek V3 from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivwcZbXDpKn4"
      },
      "source": [
        "In this article, we will go through all the math and code from scratch of a Large Language Model (specifically DeepSeek V3). We will also train our own model, **ChatSPE**, on a small dataset and chat with it. This training of a model from scratch is called pre-training. Before diving in, please note that this article is intended for readers who are familiar (not expert) with programming and have a basic understanding of machine learning concepts (activation functions, cost functions, neural networks, etc.). We will use Python and PyTorch.\n",
        "\n",
        "Below is what will be covered in this writing (with math and code):\n",
        "### Here is the big picture of LLM:\n",
        "\n",
        "1. **Text to Token IDs:**\n",
        "   *  Example: \"I love AI!\" → [1, 2, 3, 0]\n",
        "\n",
        "2. **Token IDs to Token Embeddings:**\n",
        "   *  Token IDs are just random numbers; they don't capture meaning.\n",
        "   *  Embeddings turn Token IDs into meaningful vectors, where similar words get nearby values, e.g., man ≈ 1.0, king ≈ 1.1 (same gender), while woman ≈ -1.0, queen ≈ -1.1 (opposite of male gender). In short, embeddings represent the meaning of text.\n",
        "\n",
        "3. **Token Embeddings to Q, K, V:**\n",
        "\n",
        "    *  Embeddings capture the meanings of words, but the model needs to determine how each word relates to other words in the sentence.\n",
        "    *  This is done by projecting (linear transformation of) embeddings into Query (Q), Key (K), and Value (V) vectors.\n",
        "\n",
        "    *  For example, in the sentence: *Sarah visits the bank to deposit money while Zarah visits the bank to sit by the water.*\n",
        "\n",
        "        *  **Query (Q):** Represents a specific word (e.g., bank) and asks how it relates to other words in the sentence. Is it a financial institution or a river?\n",
        "\n",
        "        *  **Key (K):** Represents all words in the sentence and is used to measure the relevance (attention score) of each word to the Query. So, \"bank\" (financial) will get a high attention score when Q is \"Sarah\".\n",
        "\n",
        "        *  **Value (V):** Contains the actual information of each word and provides the final contextual meaning. So, the word \"bank\" near \"Sarah\" means a financial institute rather than a river bank.\n",
        "\n",
        "\n",
        "4. **Positional Encodings (RoPE):**\n",
        "   *  The embeddings don't know where in the sequence the token (word) appears\n",
        "   *  Without position info, **The cat chased the dog** would look the same as **The dog chased the cat.**\n",
        "   * So we inject positional information. DeepSeek used Rotary Position Embeddings (RoPE) only for Q and K.\n",
        "\n",
        "5. **Multi-Head Latent Attention**\n",
        "6. **Mixture of Expert (MoE)**\n",
        "7. **Multi-Token Prediction (MTP)**\n",
        "8.  \n",
        "    &nbsp;&nbsp;a. **DeepSeek V3 Block (Single-Block Transformer)**  \n",
        "    &nbsp;&nbsp;b. **DeepSeek V3 Encoder (Multi-Block Transformer)**  \n",
        "    &nbsp;&nbsp;c. **DeepSeek V3 Model (Full Model)**\n",
        "9. **Pre-Training the Model**\n",
        "10. **Autoregressive Text Generation**\n",
        "11. **Fine-Tuning the Model**\n",
        "12. **Chat with your Model (Q&A)**\n",
        "13. **Deploying Model to Production**\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nh2rtNuN3CsS"
      },
      "source": [
        "## Step 1: Text to Token IDs:\n",
        "* Example: \"I love AI!\" → [1, 2, 3, 0]\n",
        "\n",
        "First, let assume a small text:\n",
        "* *I love AI so much. I enjoy learning about it, using it, and seeing how it transforms the world around us. Still, we must be ready for an AI bubble, like the dot-com crash.*\n",
        "\n",
        "We need to convert it to numbers (tokens or token IDs) as AI only works with numbers. It takes numbers (as input) and gives numbers (as output).\n",
        "\n",
        "So, we will use `tiktoken` (OpenAI's GPT-2 tokenizer) that converts a text to numbers (tokens). This converting of words to tokens is called **tokenization**.\n",
        "\n",
        "GPT-2 tokenizer (tiktoken) has a vocabulary size of 50,257. It means that tiktoken can represent 50,257 unique tokens, including words, punctuation, and special characters. Imagine it a person who memorized 50,257 English words and know how to represent them with numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbCT05uh706p"
      },
      "outputs": [],
      "source": [
        "# Below code install and import tiktokens\n",
        "! pip install -q tiktoken\n",
        "import tiktoken\n",
        "\n",
        "# Below is a small sentence\n",
        "small_text = \"\"\"I love AI so much. I enjoy learning about it, using it, and seeing how it transforms the world around us. Still, we must be ready for an AI bubble, like the dot-com crash.\"\"\"\n",
        "\n",
        "# Load GPT-2 tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Convert text -> token IDs\n",
        "token_ids = tokenizer.encode(small_text, allowed_special={\"<|endoftext|>\"})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0uhuBbUAFfF"
      },
      "source": [
        "Ok, now our `small_text` is converted to tokens (numbers). We can see the first 10 tokens by running the below code. We can also convert these numbers (tokens) back to original text by using `.decode`. See below code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLuzMV5dD7yy",
        "outputId": "87c2c5a2-45f3-41f2-a660-bc7aaef95a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First ten tokens of our small_text is:\n",
            "\n",
            "         I  ->  40\n",
            "      love  ->  1842\n",
            "        AI  ->  9552\n",
            "        so  ->  523\n",
            "      much  ->  881\n",
            "         .  ->  13\n",
            "         I  ->  314\n",
            "     enjoy  ->  2883\n",
            "  learning  ->  4673\n",
            "     about  ->  546\n"
          ]
        }
      ],
      "source": [
        "print(\"First ten tokens of our small_text is:\")\n",
        "print()\n",
        "\n",
        "for token_id in token_ids[:10]: # change 10 to a different number if you want to see more or less than 10 tokens\n",
        "    word = tokenizer.decode([token_id])\n",
        "    print(f\"{word:>10}  ->  {token_id}\") # >10 is used to adjust width for alignment\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeQ7LE3zBAgT"
      },
      "source": [
        "We can see the token_ids length (sequence length) of our training data (small_text) by running a below code. For our small_text, its 43 but you know for a real data, it would be a million or billion of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vqc3SOI9Nx3",
        "outputId": "a352c2bd-c72c-4871-da65-18ab1aacc813"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequence length  (length of token_ids) is: 43\n",
            "Type of token_ids is: <class 'list'>\n",
            "First ten values of token_ids: [40, 1842, 9552, 523, 881, 13, 314, 2883, 4673, 546]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Sequence length  (length of token_ids) is: {len(token_ids)}\")\n",
        "print(f\"Type of token_ids is: {type(token_ids)}\")\n",
        "print(f\"First ten values of token_ids: {token_ids[:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp1-ZO1HGVci"
      },
      "source": [
        "As shown above, `token_ids` is a Python list, but we convert it to a **Tensor** because ML models like Large Language Models (LLMs) accept tensor inputs that allow efficient parallel computation and gradient-based processing. We will do this by using [torch.tensor](https://docs.pytorch.org/docs/stable/tensors.html), so it will have a shape like `torch.Size([43])`.\n",
        "\n",
        "Also note that ML models always work on **batches** of sequences, where a batch is a collection of sequences processed in parallel for efficiency. So, we add a batch dimension to `token_ids`, changing its shape from `torch.Size([43])` to `torch.Size([1, 43])`, where 1 represents a single batch (later we will use larger batch size).\n",
        "\n",
        "Below code does these both (converting Python list to a Torch Tensor and adding a Batch Dimension):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXU6QVZ9E2sA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Device setup\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Convert token_ids to tensor and add batch dimension by using unsqueeze(0)\n",
        "# dtype=torch.long specifies that tensor should be a 64-bit integer\n",
        "token_ids_tensor = torch.tensor(token_ids, dtype=torch.long, device=device).unsqueeze(0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FTGbgeVIFkG"
      },
      "source": [
        "This `token_ids_tensor` has the same values as `token_ids` but now they are tensor with a batch dimension, instead of a list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIE6z3r19OGp",
        "outputId": "106213b7-847c-45bb-a80e-579545f0dcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of token_ids_tensor is: <class 'torch.Tensor'>\n",
            "Shape of token_ids_tensor is: torch.Size([1, 43])\n",
            "First ten values of token_ids_tensor: tensor([  40, 1842, 9552,  523,  881,   13,  314, 2883, 4673,  546],\n",
            "       device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "print(f\"Type of token_ids_tensor is: {type(token_ids_tensor)}\")\n",
        "print(f\"Shape of token_ids_tensor is: {token_ids_tensor.shape}\")\n",
        "print(f\"First ten values of token_ids_tensor: {token_ids_tensor[0, :10]}\") # 0 selects the first (and only) batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQpRO-uxFxNS"
      },
      "source": [
        "## Step 2: Token IDs to Token Embeddings:\n",
        "\n",
        "Alright, now we convert our token ids (token_ids_tensor) to embeddings. For this, we will use [nn.Embedding](https://docs.pytorch.org/docs/stable/generated/torch.nn.Embedding.html) that needs two input arguments, `num_embeddings` and `embedding_dim`.\n",
        "\n",
        "Here, `num_embeddings` is nothing but our vocabulary size which for GPT-2 tokenizer is 50257, and we set `embedding_dim` to a small number (4 in this case) so we can print our data in a readable form. But note that greater the `embedding_dim`, greater the accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10q33WfumvCa",
        "outputId": "406189ca-a172-409c-eb2c-d96c62762420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of embeddings is: torch.Size([1, 43, 4])\n",
            "  Here,\n",
            "    1  is the Batch Dimension\n",
            "    43 is the Sequence Length\n",
            "    4  is the Embedding Dimension\n",
            "\n",
            "Type of embeddings is: <class 'torch.Tensor'>\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Embedding setup\n",
        "vocab_size = tokenizer.n_vocab   # GPT-2 vocab size (50257)\n",
        "embedding_dim = 4    # a small random number in our case\n",
        "\n",
        "# Create an embedding layer of shape (vocab_size, embedding_dim)\n",
        "token_emb = nn.Embedding(vocab_size, embedding_dim, device=device)\n",
        "\n",
        "# Convert token IDs (token_ids_tensor) to their corresponding embedding vectors\n",
        "# Its shape is ([Batch size, sequence_length, embedding_dim]); sequence length means token_ids length\n",
        "embeddings = token_emb(token_ids_tensor)\n",
        "\n",
        "print(f\"Shape of embeddings is: {embeddings.shape}\")\n",
        "print(f\"  Here,\")\n",
        "print(f\"    {embeddings.shape[0]}  is the Batch Dimension\")\n",
        "print(f\"    {embeddings.shape[1]} is the Sequence Length\")\n",
        "print(f\"    {embeddings.shape[2]}  is the Embedding Dimension\")\n",
        "print()\n",
        "print(f\"Type of embeddings is: {type(embeddings)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYeSpn_LTay4"
      },
      "source": [
        "The code below shows the first ten words of our data, along with their token IDs and corresponding embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3jWLV23vRX6v",
        "outputId": "83b2159f-2c6a-4814-c602-5e786701110a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word  1:          I ---> Token ID: 40    ---> Embedding: [-0.9744 -0.3489 -0.0535 -1.5883]\n",
            "Word  2:       love ---> Token ID: 1842  ---> Embedding: [ 1.1393  0.9617 -0.8233 -0.5164]\n",
            "Word  3:         AI ---> Token ID: 9552  ---> Embedding: [ 0.4895  2.3509  2.0985 -1.5371]\n",
            "Word  4:         so ---> Token ID: 523   ---> Embedding: [ 1.8192 -0.6574 -0.8704  1.917 ]\n",
            "Word  5:       much ---> Token ID: 881   ---> Embedding: [0.296  0.6656 1.8575 0.0812]\n",
            "Word  6:          . ---> Token ID: 13    ---> Embedding: [0.0191 0.9065 0.3137 0.4725]\n",
            "Word  7:          I ---> Token ID: 314   ---> Embedding: [ 1.1213  0.3065 -0.2761  0.8279]\n",
            "Word  8:      enjoy ---> Token ID: 2883  ---> Embedding: [-0.6474  0.1869 -0.9054 -0.8765]\n",
            "Word  9:   learning ---> Token ID: 4673  ---> Embedding: [ 0.3439 -1.5904  0.6616  1.0657]\n",
            "Word 10:      about ---> Token ID: 546   ---> Embedding: [ 0.1021 -0.3907 -0.977   1.0673]\n"
          ]
        }
      ],
      "source": [
        "for i, (tid, emb) in enumerate(zip(token_ids_tensor[0, :10], embeddings[0,:10, :]), 1):\n",
        "    token_str = tokenizer.decode([tid])\n",
        "    emb_list = emb.detach().cpu().numpy().round(4)  # rounded for readability\n",
        "    print(f\"Word {i:>2}: {token_str:>10} ---> Token ID: {tid:<5} ---> Embedding: {emb_list}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwznPtU_raLU"
      },
      "source": [
        "## Step 3: Token Embeddings to Q, K, V:\n",
        "\n",
        "As mentioned earlier:\n",
        "\n",
        "\n",
        "  *  Embeddings capture the meanings of words, but the model needs to determine how each word relates to other words in the sentence.\n",
        "  *  This is done by projecting (linear transformation of) embeddings into Query (Q), Key (K), and Value (V) vectors.\n",
        "\n",
        "  *  For example, in the sentence: *Sarah visits the bank to deposit money while Zarah visits the bank to sit by the water.*\n",
        "\n",
        "      *  **Query (Q):** Represents a specific word (e.g., bank) and asks how it relates to other words in the sentence. Is it a financial institution or a river?\n",
        "\n",
        "      *  **Key (K):** Represents all words in the sentence and is used to measure the relevance (attention score) of each word to the Query. So, \"bank\" (financial) will get high attention score when Q is \"Sarah\".\n",
        "\n",
        "      *  **Value (V):** Contains the actual information of each word and provides the final contextual meaning. So, a word \"bank\" near \"Sarah\" means a financial institute rather than a river bank.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1Li3nQVwa5l"
      },
      "source": [
        "Let's start with embeddings:\n",
        "$$ X \\in \\mathbb{R}^{B \\times S \\times D}$$\n",
        "where\n",
        "\n",
        "* B = Batch size,\n",
        "* S = number of tokens (sequence length),\n",
        "* D = embedding dimension.\n",
        "\n",
        "## Multi-Head Attention (MHA):\n",
        "\n",
        "* Initialize weights:\n",
        "  * $$ W_q, W_k, W_v \\in \\mathbb{R}^{D \\times D} $$\n",
        "\n",
        "* So, Projections are:\n",
        "  * $$ Q = W_qX$$ $\\quad$\n",
        "  * $$K = W_kX$$  $\\quad$\n",
        "  * $$V = W_vX$$  $\\quad$\n",
        "\n",
        "* Reshape:\n",
        "    * $$\\text{Q, K, V} \\in \\mathbb{R}^{B \\times S \\times D} \\rightarrow  \\text{Q, K, V} \\in \\mathbb{R}^{B \\times H \\times S \\times d_h}$$\n",
        "\n",
        "      * where $H$ is number of head, should be divisible of $D$ (embedding dimensions)\n",
        "      * $d_h$ is the head dimension\n",
        "        * $d_h =\\frac{D}{H}$\n",
        "\n",
        "* Finally, attention is:\n",
        "  * $$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "    \n",
        "    * where $d_k$ is the key dimension (which is equal to head dimention):\n",
        "      * $d_k = d_h =\\frac{D}{H}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH-FBkh7nBSG",
        "outputId": "8aa98231-007f-49c9-b3b0-caf3a45f1798"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(B, S, D)\n",
            "Q shape: torch.Size([1, 43, 4])\n",
            "K shape: torch.Size([1, 43, 4])\n",
            "V shape: torch.Size([1, 43, 4])\n",
            "\n",
            "(B, H, S, d_head)\n",
            "Q shape: torch.Size([1, 2, 43, 2])\n",
            "K shape: torch.Size([1, 2, 43, 2])\n",
            "V shape: torch.Size([1, 2, 43, 2])\n",
            "\n",
            "(B, H, S, d_head)\n",
            "Attention shape: torch.Size([1, 2, 43, 2])\n"
          ]
        }
      ],
      "source": [
        "# embeddings shape: (B, S, embedding_dims)\n",
        "# In the Attention is all you need, authors used `d_model` which is the same as `embedding_dims`.\n",
        "batch_size, seq_len, embedding_dims = embeddings.shape\n",
        "\n",
        "num_heads = 2 # number of heads\n",
        "\n",
        "d_head = embedding_dims // num_heads # head dimension\n",
        "assert embedding_dims % num_heads == 0, f\"embedding_dims = {embedding_dims} must be divisible by num_heads = {num_heads}\"\n",
        "\n",
        "\n",
        "# Linear projections for Q, K, V\n",
        "W_q = nn.Linear(embedding_dims, embedding_dims, bias=False, device=device)\n",
        "W_k = nn.Linear(embedding_dims, embedding_dims, bias=False, device=device)\n",
        "W_v = nn.Linear(embedding_dims, embedding_dims, bias=False, device=device)\n",
        "\n",
        "# Apply projections\n",
        "Q = W_q(embeddings) # (B, S, D)\n",
        "# Pytorch internall do this: Q = embeddings @ W_q.weight.T\n",
        "K = W_k(embeddings) # (B, S, D)\n",
        "V = W_v(embeddings) # (B, S, D)\n",
        "\n",
        "\n",
        "print(\"(B, S, D)\")\n",
        "print(\"Q shape:\", Q.shape)  # (B, S, D)\n",
        "print(\"K shape:\", K.shape)  # (B, S, D)\n",
        "print(\"V shape:\", V.shape)  # (B, S, D)\n",
        "print()\n",
        "\n",
        "# Reshape and transpose to split projections into separate heads (B, H, S, D), which is the format attention needs.\n",
        "# Reshape: (B, S, D) -> (B, H, S, d_head)\n",
        "Q = Q.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
        "K = K.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
        "V = V.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
        "\n",
        "print(\"(B, H, S, d_head)\")\n",
        "print(\"Q shape:\", Q.shape)  # (B, H, S, d_head)\n",
        "print(\"K shape:\", K.shape)  # (B, H, S, d_head)\n",
        "print(\"V shape:\", V.shape)  # (B, H, S, d_head)\n",
        "print()\n",
        "\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "\n",
        "scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_head)\n",
        "\n",
        "# Softmax over last dimension (S)\n",
        "attn_weights = F.softmax(scores, dim=-1)   # (B, H, S, S)\n",
        "\n",
        "# (B, H, S, S) @ (B, H, S, d_head) -> (B, H, S, d_head)\n",
        "attention = torch.matmul(attn_weights, V)\n",
        "\n",
        "print(\"(B, H, S, d_head)\")\n",
        "print(\"Attention shape:\", attention.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWdTWsIdYiXk"
      },
      "source": [
        "The above code is a plain code, just to have a clear understanding how attention mechanism works. In Step 5, we will do the Multi-Head Latent Attention in a more sophisticated way."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8Pt6aTzSIJ6"
      },
      "source": [
        "# Step 4: Positional Encodings:\n",
        "\n",
        "As mentioned earlier, without positional information, “The cat chased the dog” is the same as “The dog chased the cat.” To encode order, positional information is injected.\n",
        "\n",
        "DeepSeek applies Rotary Position Embeddings (RoPE) to the query Q and key K vectors only.\n",
        "\n",
        "\n",
        "### Rotary Position Embedding (RoPE):\n",
        "\n",
        "**Step 1. Split Q and K into 2D subspaces**\n",
        "\n",
        "* Split Q and K into pairs of dimensions (below is for Q):\n",
        "\n",
        "  * $Q = [(q_1, q_2),(q_3, q_4),...,(q_{d-1}, q_d)]$\n",
        "\n",
        "**Step 2. Compute rotation frequencies (per dimension pair)**\n",
        "\n",
        "* Assign a rotation frequency $\\theta_i$ for each 2D pair:\n",
        "  * $\\theta_i = \\frac {1}{10000^{\\frac{2i}{d}}}  = 10000^{-\\frac{2i}{d}}$\n",
        "  * where:\n",
        "    * i is the index of each 2D dimension pair\n",
        "    * d is head dimension\n",
        "\n",
        "**Step 3. Compute rotation angles (per position and dimension pair)**\n",
        "\n",
        "  * $ \\phi_{m,i} =  m. \\theta_i$\n",
        "  * where:\n",
        "    * m is the position index (of a word/token) in the sequence\n",
        "\n",
        "**Step 4. Construct Rotation Matrix**\n",
        "* $R(\\phi_{m,i}) = \\begin{bmatrix} cos(\\phi_{m,i}) & -sin(\\phi_{m,i})  \\\\ sin(\\phi_{m,i}) & cos(\\phi_{m,i}) \\end{bmatrix}$\n",
        "\n",
        "**Step 5. Apply rotation to each 2D subspace**\n",
        "*  $ \\tilde{q}_{2i-1} = q_{2i-1} cos(\\phi_{m,i}) - \\tilde{q}_{2i} sin(\\phi_{m,i}) $\n",
        "\n",
        "*  $ \\tilde{q}_{2i} = q_{2i-1} sin(\\phi_{m,i}) + \\tilde{q}_{2i} cos(\\phi_{m,i}) $\n",
        "* where:\n",
        "  * $ q_{2i-1}$ is odd-indexed component (first element of pair)\n",
        "\n",
        "  * $ q_{2i}$ is even-indexed component (second element of pair)\n",
        "* In Matrix form, it is\n",
        "\n",
        "  * $ \\begin{bmatrix} \\tilde{q}_{2i-1} \\\\ \\tilde{q}_{2i} \\end{bmatrix}=  \\begin{bmatrix} cos(\\phi_{m,i}) & -sin(\\phi_{m,i})  \\\\ sin(\\phi_{m,i}) & cos(\\phi_{m,i}) \\end{bmatrix} \\begin{bmatrix} q_{2i-1} \\\\ q_{2i} \\end{bmatrix} $\n",
        "  \n",
        "\n",
        "**Step 6: Concatenate $\\tilde{q}_{2i-1} \\text{ and } \\tilde{q}_{2i} \\text{ into }\\tilde{Q}$**\n",
        "\n",
        "\n",
        "* $\\tilde{Q} = \\left[\\tilde{q}_{2i-1}, \\tilde{q}_{2i}  \\right] = \\left[\\tilde{q}_{1}, \\tilde{q}_{2}, \\tilde{q}_{3},...,\\tilde{q}_{d-1}, \\tilde{q}_{d}  \\right]$\n",
        "\n",
        "* Use $\\tilde{Q}$ (and $\\tilde{K}$) in attention, instead of $Q$ and $K$.\n",
        "___\n",
        "References:\n",
        "\n",
        "* [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)\n",
        "\n",
        "* [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/html/2405.04434)\n",
        "___"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WY3GtIRWvVQg"
      },
      "outputs": [],
      "source": [
        "class Rope(nn.Module):\n",
        "\n",
        "    \"\"\"\n",
        "    Rotary Position Embedding (RoPE) module.\n",
        "    Applies rotary position encoding to an input tensor of shape (B, H, S, D),\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, max_seq_len = 4096):\n",
        "        super().__init__()\n",
        "\n",
        "        # Safety check: RoPE requires even dimensionality (for splitting into pairs)\n",
        "        assert dim % 2 == 0, f\"RoPE dim must be even, got {dim}\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Step 2: Compute rotation frequencies for sinusoidal positions\n",
        "        # inv_freq[i] = 1 / (10000^(2i/dim)), where i = 0, 1, ..., dim/2 - 1\n",
        "        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
        "\n",
        "        # Store as non-trainable buffer\n",
        "        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n",
        "\n",
        "        # Precompute and cache cos/sin values up to max_seq_len\n",
        "        self._build_cache(max_seq_len)\n",
        "\n",
        "    def _build_cache(self, seq_len):\n",
        "        \"\"\"\n",
        "        Precompute cosine and sine embeddings for all positions up to seq_len.\n",
        "        This avoids recomputing trig functions during every forward pass.\n",
        "        \"\"\"\n",
        "\n",
        "        # Positions: [0, 1, 2, ..., seq_len-1]\n",
        "        t = torch.arange(seq_len, dtype=self.inv_freq.dtype, device=self.inv_freq.device)\n",
        "\n",
        "        # Step 3: Compute rotation angles (per position and dimension pair)\n",
        "        # Each row is t * inv_freq[i], giving angular frequency per dimension\n",
        "        freqs = torch.outer(t, self.inv_freq)\n",
        "\n",
        "        # Duplicate for concatenation of sin and cos values, shape: (seq_len, dim)\n",
        "        emb = torch.cat((freqs, freqs), dim=-1)\n",
        "\n",
        "        # Step 4: Construct rotation matrix elements (cos and sin)\n",
        "        # Register as buffers with shape (1, 1, seq_len, dim)\n",
        "        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n",
        "        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n",
        "\n",
        "        # Track how many positions we have cached\n",
        "        self.max_seq_len = seq_len\n",
        "\n",
        "    def forward(self, x, seq_len, position_offset = 0):\n",
        "\n",
        "        \"\"\"\n",
        "        Apply RoPE to input tensor.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (B, H, S, D)\n",
        "            seq_len: Actual sequence length to encode\n",
        "            position_offset: Offset for decoding continuation (default = 0)\n",
        "\n",
        "        Returns:\n",
        "            Tensor with RoPE applied, same shape as x.\n",
        "        \"\"\"\n",
        "\n",
        "        device = x.device\n",
        "\n",
        "        # Ensure input matches expected dimensionality\n",
        "        assert x.shape[-1] == self.dim, (\n",
        "            f\"RoPE input dim mismatch: expected {self.dim}, got {x.shape[-1]}\"\n",
        "        )\n",
        "\n",
        "        seq_len_x = x.size(-2)  # sequence length from input tensor\n",
        "\n",
        "        if (position_offset + seq_len) > self.max_seq_len:\n",
        "            # Rebuild cache with doubled size for efficiency\n",
        "            self._build_cache(max(position_offset + seq_len, self.max_seq_len * 2))\n",
        "\n",
        "        # Select only the needed positions\n",
        "        cos = self.cos_cached[:, :, position_offset:position_offset + seq_len, :].to(device)\n",
        "        sin = self.sin_cached[:, :, position_offset:position_offset + seq_len, :].to(device)\n",
        "\n",
        "        # Ensure cache slice matches actual input sequence length\n",
        "        assert cos.shape[2] == seq_len_x, (\n",
        "            f\"RoPE seq_len mismatch: expected {seq_len_x}, got {cos.shape[2]}\"\n",
        "        )\n",
        "\n",
        "        # Step 1: Split Q/K into 2D subspaces (pairs of dimensions)\n",
        "        # Split last dimension into pairs: (x1, x2)\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "\n",
        "        # Step 5 and Step 6: Apply rotation to each 2D subspace\n",
        "        # Rotate pairs: (x1, x2) → (-x2, x1)\n",
        "        rotated = torch.cat((-x2, x1), dim=-1)\n",
        "        # Apply rotary transformation: elementwise (x*cos + rotated*sin)\n",
        "        result = x*cos + rotated*sin\n",
        "\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slohNl8Htb4H"
      },
      "source": [
        "The above `Rope` code can be used as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3ZLnwhp5NTf",
        "outputId": "9f9d137a-af1d-4a75-db66-abe40a0d7881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# Rope\n",
        "\n",
        "# initialize RoPE with head dimention (d_head)\n",
        "rope = Rope(dim=d_head)\n",
        "\n",
        "# Apply RoPE to query Q and key K vectors\n",
        "Q_rot = rope(Q, seq_len = seq_len)\n",
        "K_rot = rope(K, seq_len = seq_len)\n",
        "\n",
        "print(f\"Run successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEV2pTvi5ape"
      },
      "source": [
        "Below code prints the values of Q and K before and after applying the `RoPE`. You can see the values are different, meaning that the we have encoded positional information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwYn-6zT8IjP",
        "outputId": "6bc86d10-c3e9-4004-b45e-c390907f2f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Q_before_RoPE  Q_after_RoPE  K_before_RoPE  K_after_RoPE\n",
            "0        -1.060441     -1.060441       0.271295      0.271295\n",
            "1         0.607195      0.607195      -0.945203     -0.945203\n",
            "2         0.277562      0.342832       0.418060      0.097800\n",
            "3        -0.229200      0.109723       0.152208      0.434024\n",
            "4        -0.724682     -1.064678      -0.559731      1.031844\n",
            "..             ...           ...            ...           ...\n",
            "167       0.454738     -0.835753       0.152602      0.162784\n",
            "168      -0.379715      0.382291      -0.739499      0.666359\n",
            "169       0.046551      0.014269      -0.402069      0.514280\n",
            "170       0.337109     -0.612003      -0.040325      0.031864\n",
            "171      -0.520625     -0.100726       0.017168      0.030091\n",
            "\n",
            "[172 rows x 4 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Flatten and convert Q and Q_rot to numpy\n",
        "Q_before = Q.detach().cpu().numpy().reshape(-1)\n",
        "Q_after  = Q_rot.detach().cpu().numpy().reshape(-1)\n",
        "\n",
        "# Flatten and convert K and K_rot to numpy\n",
        "K_before = K.detach().cpu().numpy().reshape(-1)\n",
        "K_after  = K_rot.detach().cpu().numpy().reshape(-1)\n",
        "\n",
        "# Combine into one DataFrame with 4 columns\n",
        "df_combined = pd.DataFrame({\n",
        "    \"Q_before_RoPE\": Q_before,\n",
        "    \"Q_after_RoPE\": Q_after,\n",
        "    \"K_before_RoPE\": K_before,\n",
        "    \"K_after_RoPE\": K_after\n",
        "})\n",
        "\n",
        "# Print the horizontal table\n",
        "print(df_combined)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_11OdNP2KiC"
      },
      "source": [
        "# Step 5: Multi-Head Latent Attention\n",
        "\n",
        "This is very important step, so read it carefully.\n",
        "\n",
        "Again, we start with embeddings:\n",
        "$$ X \\in \\mathbb{R}^{B \\times S \\times D}$$\n",
        "where\n",
        "\n",
        "* B = Batch size,\n",
        "* S = number of tokens (sequence length),\n",
        "* D = embedding dimension.\n",
        "\n",
        "### Multi-Head Latent Attention (MLA) (Latent Compression):\n",
        "\n",
        "**For Training:**\n",
        "\n",
        "* $$ W_{DKV},W_{DQ} \\in \\mathbb{R}^{D \\times d_l}, \\quad $$  \n",
        "  * where $d_l$ is the latend dimension ($d_l<< D $) $$\\quad$$\n",
        "* $$ c_{kv} = W_{DKV}X, \\quad c_{q} = W_{DQ}X $$ $$\\quad$$\n",
        "* $$ W_{UQ}, W_{UK}, W_{UV}  \\in \\mathbb{R}^{d_l \\times D} $$ $$\\quad$$\n",
        "* $$ Q_c = W_{UQ}c_{q}, \\quad K_c = W_{UK}c_{kv}, \\quad V_c = W_{UV}c_{kv} $$ $$\\quad$$\n",
        "* $$ W_{KR} \\in \\mathbb{R}^{D \\times d_R}, \\quad \\quad W_{QR} \\in \\mathbb{R}^{d_l \\times (H \\times d_R)}, \\quad \\quad $$\n",
        "  * where $d_R$ is the positional embedding dimension and $H$ is the number of heads $$\\quad$$\n",
        "* $$ K_R = W_{KR}X, \\quad Q_R = W_{QR}X $$\n",
        "* $$ K_R = RoPE(K_R), \\quad Q_R = RoPE(Q_R) $$ $$\\quad$$\n",
        "* $$ K = [k_C, K_R] \\quad Q = [Q_C, Q_R]$$ $$\\quad$$\n",
        "* $$\\text{Attention} = o = \\text{ softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V_c$$\n",
        "\n",
        "  * where $d_k$ is the (head dimention)  + (positional embedding dimension), e.g., $d_k = d_h + d_R$\n",
        "\n",
        "So, final output is:\n",
        "\n",
        "* $$ y = W_O[o_1, o_2,...o_h], \\quad \\text{ where, }  W_O \\in \\mathbb{R}^{D \\times D}$$\n",
        "\n",
        "**For Inference:**\n",
        "\n",
        "\n",
        "* $$ Q_l =  W_{UK}Q_C$$\n",
        "* $$ \\hat{Q} = [Q_l, Q_R] $$\n",
        "* $$ \\hat{K} = [c_{kv}, K_R] $$\n",
        "* $$\\text{Attention} = \\hat{o} = \\text{ softmax}\\left(\\frac{\\hat{Q^T}\\hat{K}}{\\sqrt{d_k}}\\right)c_{kv}$$\n",
        "\n",
        "So, final output is:\n",
        "\n",
        "* $$ y = W_O[W_{UV_1} \\hat{o_1}, W_{UV_2}\\hat{o_2},...W_{UV_h}\\hat{o_h}] $$\n",
        "___\n",
        "Reference: [TransMLA: MLA Is All You Need](https://arxiv.org/html/2502.07864) (Section 3.3: Multi-Head Latent Attention).\n",
        "___\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLtyjcIFBe5U"
      },
      "source": [
        "Below is the DeepSeek MLA code implementing the mathematics we have just discussed. It also includes `use_cache` and `past_key_values` for faster inference, and an `attention_mask` to properly handle padding and causality (so the model ignores padded and future tokens). If you are not familiar with these terms, read a quick explanation below:\n",
        "\n",
        "* **use_cache:** Use when generating text autoregressively (one token at a time). For instance, first the model outputs two words, “The cat”, then we ask it for the next word (third word); setting `use_cache = True` makes the model store the computations for “The cat” inside `past_key_values` so it does not recompute them.\n",
        "\n",
        "* **past_key_values:** This is the stored result created when `use_cache = True`. For example, if we already have “The cat” and want the model to continue, we pass in `past_key_values` so it continues without reprocessing “The cat.”\n",
        "  * *Difference between `use_cache` and `past_key_values`:* `use_cache` is the sign that tells the model to save computations, `past_key_values` is the actual saved data used in subsequent steps.\n",
        "\n",
        "* **attention_mask:** Use when batching sentences of different lengths, like “The cat sat” and “The dog”; it masks/hides padding (e.g., “The dog [PAD]”) so the model does not attend to the padding. It also hides future tokens to ensure predictions only depend on past and current words.\n",
        "\n",
        "If these jargons are still unclear, no worries, you can safely skip them without losing the main understanding; full details are beyond the scope of this guide."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a46aArXO8nTc"
      },
      "outputs": [],
      "source": [
        "class DeepSeek_MLA(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size # Embedding dimension\n",
        "        self.num_heads = config.num_heads\n",
        "        self.head_dim = self.hidden_size // self.num_heads\n",
        "        self.latent_dim = config.latent_dim\n",
        "        self.pos_dim = config.pos_dim\n",
        "        self.max_seq_len = getattr(config, 'max_seq_len', 512)  # Add max sequence length\n",
        "        self.pad_token_id = getattr(config, 'pad_token_id', 50256)  # Default pad token ID to 50256 of tiktoken’s GPT-2 vocab, same as <|endoftext|> in the tiktoken’s GPT-2 vocab\n",
        "\n",
        "\n",
        "        assert self.hidden_size % self.num_heads == 0, f\"hidden_size ({self.hidden_size}) must be divisible by num_heads ({self.num_heads})\"\n",
        "        # Ensure pos_dim is even for RoPE\n",
        "        assert self.pos_dim % 2 == 0, f\"pos_dim ({self.pos_dim}) must be even for RoPE\"\n",
        "\n",
        "        # Latent compression projections\n",
        "        self.W_DKV = nn.Linear(self.hidden_size, self.latent_dim, bias=False)  # KV compression\n",
        "        self.W_DQ = nn.Linear(self.hidden_size, self.latent_dim, bias=False)   # Q compression\n",
        "\n",
        "        # Content projection from latent to multi-head space\n",
        "        self.W_UK = nn.Linear(self.latent_dim, self.hidden_size, bias=False)  # K content\n",
        "        self.W_UV = nn.Linear(self.latent_dim, self.hidden_size, bias=False)  # V content\n",
        "        self.W_UQ = nn.Linear(self.latent_dim, self.hidden_size, bias=False)  # Q content\n",
        "\n",
        "        # Positional projections (RoPE pathway)\n",
        "        self.W_KR = nn.Linear(self.hidden_size, self.pos_dim, bias=False)     # K positional\n",
        "        self.W_QR = nn.Linear(self.latent_dim, self.num_heads * self.pos_dim, bias=False)  # Q positional\n",
        "\n",
        "        # Output projection\n",
        "        self.W_O = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "\n",
        "        # RoPE initialization\n",
        "        self.rope_k = Rope(self.pos_dim)\n",
        "        self.rope_q = Rope(self.pos_dim)\n",
        "\n",
        "        # ---- Precomputed causal mask ----\n",
        "        # Create upper triangular mask with ones above diagonal and convert to boolean\n",
        "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(self.max_seq_len, self.max_seq_len), diagonal=1).bool())\n",
        "\n",
        "    def forward(self, hidden_states, input_tokens=None, mode=\"train\", use_cache=False, past_key_values=None, attention_mask=False):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
        "        assert hidden_size == self.hidden_size, f\"hidden_size mismatch: got {hidden_size}, expected {self.hidden_size}\"\n",
        "\n",
        "        # ---- Latent compressions ----\n",
        "        c_KV = self.W_DKV(hidden_states)   # (batch_size, seq_len, latent_dim)\n",
        "        c_Q  = self.W_DQ(hidden_states)    # (batch_size, seq_len, latent_dim)\n",
        "\n",
        "        # ---- Content projections (per-head) ----\n",
        "        k_C = self.W_UK(c_KV).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, H, seq_len, head_dim)\n",
        "        v_C = self.W_UV(c_KV).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)  # (batch_size, H, seq_len, head_dim)\n",
        "        q_C = self.W_UQ(c_Q).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)   # (batch_size, H, seq_len, head_dim)\n",
        "\n",
        "        # ---- Positional projections ----\n",
        "        k_R = self.W_KR(hidden_states)   # (batch_size, seq_len, pos_dim)\n",
        "        q_R = self.W_QR(c_Q).view(batch_size, seq_len, self.num_heads, self.pos_dim).transpose(1, 2)  # (batch_size, H, seq_len, pos_dim)\n",
        "\n",
        "        # ---- Determine past length for RoPE position_offset ----\n",
        "        past_len = 0 if past_key_values is None else past_key_values[0].size(2)\n",
        "\n",
        "        # ---- Apply RoPE (position offset = past_len) ----\n",
        "        k_R = self.rope_k(k_R.unsqueeze(1).expand(-1, self.num_heads, -1, -1), seq_len=seq_len, position_offset=past_len)  # (batch_size, H, seq_len, pos_dim)\n",
        "        q_R = self.rope_q(q_R, seq_len=seq_len, position_offset=past_len)  # (batch_size, H, seq_len, pos_dim)\n",
        "\n",
        "        ######### TRAINING MODE #########\n",
        "\n",
        "        if mode == \"train\":\n",
        "            k = torch.cat([k_C, k_R], dim=-1)  # (batch_size, H, seq_len, head_dim + pos_dim)\n",
        "            q = torch.cat([q_C, q_R], dim=-1)  # (batch_size, H, seq_len, head_dim + pos_dim)\n",
        "\n",
        "            scale = 1.0 / math.sqrt(q.shape[-1]) # same as scale = 1.0 / math.sqrt(head_dim + pos_dim)\n",
        "            attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # (batch_size, H, seq_len, seq_len)\n",
        "\n",
        "            # ---- Apply mask (causal + padding) ----\n",
        "            if attention_mask:\n",
        "                # Mask truncated to the number of tokens and converted to boolean\n",
        "                mask_bool = self.causal_mask[:seq_len, :seq_len]\n",
        "\n",
        "                # Convert boolean mask to -inf format for attention\n",
        "                causal_mask = mask_bool.float().masked_fill(mask_bool, float('-inf'))\n",
        "\n",
        "                # Create padding mask from hidden states\n",
        "                padding_mask = (input_tokens == self.pad_token_id) #.all(dim=-1)  # (B, S) - True where all features are 50256\n",
        "\n",
        "                # Expand padding mask to match attention scores shape\n",
        "                padding_mask = padding_mask.unsqueeze(1).unsqueeze(2)  # (B, 1, 1, S)\n",
        "                padding_mask = padding_mask.expand(-1, self.num_heads, seq_len, -1)  # (B, H, S, S)\n",
        "                padding_mask = padding_mask.float().masked_fill(padding_mask, float('-inf'))\n",
        "\n",
        "                # Combine causal and padding masks\n",
        "                full_mask = causal_mask.unsqueeze(0).unsqueeze(0) + padding_mask\n",
        "                attn_scores = attn_scores + full_mask\n",
        "\n",
        "\n",
        "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "            o_heads = torch.matmul(attn_probs, v_C)  # (batch_size, H, seq_len, head_dim)\n",
        "\n",
        "            kv_cache = None  # training returns no cache\n",
        "\n",
        "        ######### INFERENCE MODE #########\n",
        "\n",
        "        elif mode == \"inference\":\n",
        "            # Concatenate past and current per-head keys/values/pos if provided\n",
        "            if past_key_values is None:\n",
        "                k_C_total = k_C  # (batch_size, H, seq_len, head_dim)\n",
        "                v_C_total = v_C\n",
        "                k_R_total = k_R\n",
        "                q_R_total = q_R\n",
        "                c_KV_total = c_KV.unsqueeze(1).expand(-1, self.num_heads, -1, -1)  # (batch_size, H, seq_len, latent_dim)\n",
        "                total_len = seq_len\n",
        "            else:\n",
        "                # past_key_values: (past_k_cache, past_v_cache, past_kR_cache, past_qR_cache)\n",
        "                past_k_cache, past_v_cache, past_k_R_cache, past_q_R_cache, past_c_KV_total = past_key_values\n",
        "                # Append along sequence dim (dim=2 for per-head)\n",
        "                k_C_total = torch.cat([past_k_cache, k_C], dim=2)   # (batch_size, H, past_len+seq_len, head_dim)\n",
        "                v_C_total = torch.cat([past_v_cache, v_C], dim=2)\n",
        "                k_R_total = torch.cat([past_k_R_cache, k_R], dim=2)  # (batch_size, H, past_len+seq_len, pos_dim)\n",
        "                q_R_total = torch.cat([past_q_R_cache, q_R], dim=2)\n",
        "                c_KV_total = torch.cat([past_c_KV_total, c_KV.unsqueeze(1).expand(-1, self.num_heads, -1, -1)], dim=2) # (batch_size, H, total_len, latent_dim)\n",
        "                total_len = k_C_total.size(2)\n",
        "\n",
        "\n",
        "            # q_latent computation\n",
        "            W_UK_heads = self.W_UK.weight.view(self.num_heads, self.head_dim, self.latent_dim)\n",
        "            q_latent = torch.matmul(q_C, W_UK_heads)  # (batch, heads, seq_len, latent_dim)\n",
        "\n",
        "            k_hat = torch.cat([c_KV_total, k_R_total], dim=-1)\n",
        "            q_hat = torch.cat([q_latent, q_R], dim=-1)  # (batch_size, H, seq_len, head_dim+pos_dim)\n",
        "\n",
        "            # Attention\n",
        "            scale = 1.0 / math.sqrt(k_hat.shape[-1])\n",
        "            attn_scores = torch.matmul(q_hat, k_hat.transpose(-2, -1)) * scale  # (batch_size, H, seq_len, total_len)\n",
        "\n",
        "            # ---- Apply mask (causal + padding, cache-aware) ----\n",
        "            if attention_mask:\n",
        "\n",
        "                mask_bool = self.causal_mask[:total_len, :total_len]\n",
        "                causal_mask_base = mask_bool.float().masked_fill(mask_bool, float('-inf'))\n",
        "\n",
        "                offset = total_len - seq_len\n",
        "                causal_mask = causal_mask_base[offset:offset+seq_len, :total_len].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "                causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, total_len)\n",
        "\n",
        "                # Create padding mask from hidden states\n",
        "                padding_mask = (input_tokens == self.pad_token_id) #.all(dim=-1)  # (B, S) - True where all features are 50256\n",
        "\n",
        "                # For inference with cache, we need to handle the full sequence length\n",
        "                padding_mask_full = torch.zeros(batch_size, total_len, device=hidden_states.device, dtype=torch.bool)\n",
        "                padding_mask_full[:, -seq_len:] = padding_mask  # Only the current tokens have padding\n",
        "\n",
        "                padding_mask_expanded = padding_mask_full.unsqueeze(1).unsqueeze(2).expand(-1, self.num_heads, seq_len, -1)\n",
        "                padding_mask_expanded = padding_mask_expanded.float().masked_fill(padding_mask_expanded, float('-inf'))\n",
        "\n",
        "                full_mask = causal_mask + padding_mask_expanded\n",
        "                attn_scores = attn_scores + full_mask\n",
        "\n",
        "\n",
        "            attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "            o_hat = torch.matmul(attn_probs, c_KV_total)  # (batch_size, H, seq_len, latent_dim)\n",
        "\n",
        "            # 2. Apply per-head W_UV projection (Absorb step)\n",
        "            W_UV_heads = self.W_UV.weight.view(self.num_heads, self.head_dim, self.latent_dim) # [H, head_dim, latent_dim]\n",
        "            o_heads = torch.matmul(o_hat, W_UV_heads.transpose(1, 2))  # [batch_size, H, seq_len, head]\n",
        "\n",
        "            # Prepare kv_cache tuple to return (present caches covering full sequence)\n",
        "            if use_cache:\n",
        "                kv_cache = (\n",
        "                    k_C_total.detach(),\n",
        "                    v_C_total.detach(),\n",
        "                    k_R_total.detach(),\n",
        "                    q_R_total.detach(),\n",
        "                    c_KV_total.detach()\n",
        "                )\n",
        "            else:\n",
        "                kv_cache = None\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"mode must be 'train' or 'inference'\")\n",
        "\n",
        "        # ---- Final projection ----\n",
        "        o = o_heads.transpose(1, 2).reshape(batch_size, seq_len, self.num_heads * self.head_dim)  # (batch_size, seq_len, hidden_size)\n",
        "        attn_output = self.W_O(o)  # (batch_size, seq_len, hidden_size)\n",
        "\n",
        "        return attn_output, kv_cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Av8-2hVJ7MU"
      },
      "source": [
        "The above `DeepSeek_MLA` code can be used as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfcAP2LZB5xA",
        "outputId": "22cdb569-272d-4e14-e2e0-62bd27e1e3db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Train pass (no cache) ---\n",
            "Train Input shape : torch.Size([1, 43, 4])\n",
            "Train output shape: torch.Size([1, 43, 4])\n",
            "\n",
            "--- Inference Pass 1 (no cache) ---\n",
            "Train Input shape : torch.Size([1, 43, 4])\n",
            "Train output shape: torch.Size([1, 43, 4])\n",
            "\n",
            "--- Inference Pass 2 (with cache) ---\n",
            "Train Input shape : torch.Size([1, 43, 4])\n",
            "Train output shape: torch.Size([1, 43, 4])\n",
            "\n",
            "--- Inference Pass 3 (with cache) ---\n",
            "Train Input shape : torch.Size([1, 43, 4])\n",
            "Train output shape: torch.Size([1, 43, 4])\n",
            "\n",
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek_MLA\n",
        "\n",
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Initialize DeepSeek_MLA module with the above config\n",
        "attention = DeepSeek_MLA(config).to(device)\n",
        "\n",
        "# Training pass (no caching)\n",
        "print(\"\\n--- Train pass (no cache) ---\")\n",
        "train_out, kv_cache = attention(\n",
        "    embeddings,\n",
        "    input_tokens=token_ids_tensor,\n",
        "    mode=\"train\",\n",
        "    use_cache=False,            # Do not store past_key_values\n",
        "    attention_mask=True          # Mask padding & future tokens\n",
        ")\n",
        "print(\"Train Input shape :\", embeddings.shape)\n",
        "print(\"Train output shape:\", train_out.shape)\n",
        "\n",
        "# Inference passes\n",
        "attention.eval() # Set module to evaluation mode\n",
        "for i in range(3):\n",
        "    label = \"no cache\" if i == 0 else \"with cache\"\n",
        "    print(f\"\\n--- Inference Pass {i+1} ({label}) ---\")\n",
        "\n",
        "    attn_out, kv_cache = attention(\n",
        "        embeddings,\n",
        "        input_tokens=token_ids_tensor,\n",
        "        mode=\"inference\",\n",
        "        use_cache=True,            # Enable caching for faster autoregressive inference\n",
        "        past_key_values=kv_cache,  # Feed previously cached attention keys/values\n",
        "        attention_mask=True        # Mask padding & future tokens\n",
        "    )\n",
        "\n",
        "    print(\"Train Input shape :\", embeddings.shape)\n",
        "    print(\"Train output shape:\", train_out.shape)\n",
        "\n",
        "print(f\"\\nRun successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu2lzGufkQFR"
      },
      "source": [
        "# Step 6: Mixture of Experts (MoE)\n",
        "Again, this is very important step, so read it carefully.\n",
        "\n",
        "A Mixture of Experts is like a team of small neural networks (experts).\n",
        "Instead of one large network doing everything, each expert specializes in a certain type of data.\n",
        "\n",
        "When a new input arrives:\n",
        "\n",
        "* A router (or gating network) decides which experts should handle it.\n",
        "\n",
        "* Only those few experts are used (saves computation).\n",
        "\n",
        "* Their outputs are combined to form the final result.\n",
        "\n",
        "## Types of Neural Networks (NN) in MoE\n",
        "\n",
        "\n",
        "1. **Router** (gating network): This is a small linear layer that looks at the input and decides which experts should be used for that token.\n",
        "\n",
        "2. **Shared Experts** (general knowledge): These are always active and learn general, global information that helps all tokens.\n",
        "\n",
        "3. **Routed Experts** (specialized brains): These are many small expert networks trained to handle different parts of the input space. They do not all activate at once — the router picks which ones to use.\n",
        "\n",
        "$\\quad$\n",
        "\n",
        "## More Details\n",
        "\n",
        "For Input $X$:\n",
        "\n",
        "* $$ X \\in \\mathbb{R}^{B \\times S \\times D}$$\n",
        "    \n",
        "    * B = Batch size,\n",
        "    * S = number of tokens (sequence length),\n",
        "    * D = embedding dimension.\n",
        "\n",
        "$\\quad$\n",
        "\n",
        "1. **Shared Experts** (a few small networks) activated all the time:\n",
        "* $$O_{sh} = \\sum_{i=1}^{N_{s}} FFN_i^{(s)}(X_t)$$\n",
        "  * where\n",
        "    * $O_{s}$ = shared experts output\n",
        "    * $N_{s}$ = number of shared experts\n",
        "    * $ X_t$ = input of the $t^{th}$ token\n",
        "    * $FNN_i^{(s)}(\\cdot)$ = $i^{th}$ shared expert neural network\n",
        "\n",
        "$\\quad$\n",
        "\n",
        "2. **Routed Experts** (many small networks) activated selectively:\n",
        "* $$O_{r} = \\sum_{i=1}^{N_{r}} g_{i,t} FFN_i^{(r)}(X_t)$$\n",
        "  * where\n",
        "    * $O_{r}$ = routed experts output\n",
        "    * $N_{r}$ = number of routed experts\n",
        "    * $ X_t$ = input of the $t^{th}$ token\n",
        "    * $FNN_i^{(r)}(\\cdot)$ = $i^{th}$ routed expert neural network\n",
        "    * $g_{i,t}$ is the gating score for the $i^{th}$ expert (explain below)\n",
        "\n",
        "$\\quad$\n",
        "\n",
        "3. **Gating Scores with Load Balancing** (only for routed experts):\n",
        "* $$ g_{i,t} = \\frac {g'_{i,t}}{\\sum_{j=1}^{N_r} g'_{j,t}} $$\n",
        "\n",
        "* $$g'_{i,t} = \\begin{cases} s_{i,t}, & \\text{if } s_{i,t} + b_i \\in \\text{Topk}\\Big(\\{s_{j,t} + b_j \\mid 1 \\le j \\le N_r\\}, K_r\\Big), \\\\\n",
        "0, & \\text{otherwise.} \\end{cases}$$\n",
        "$\\quad$\n",
        "* $$ s_{i,t} = Sigmoid(u_t^T e_i) $$\n",
        "  * where\n",
        "    * $g_{i,t}$ is the gating value for expert $i$ at token $t$\n",
        "    * $s_{i,t}$ is the affinity score (how relevant expert $i$ is for token $t$).\n",
        "    * $b_i$ is the bias for load balancing of expert $i$.\n",
        "    * $e_i$ is the centroid vector of the $i^{th}$ routed expert\n",
        "    * The Top-K selection picks the $K_r$ most relevant experts for token $t$ based on affinity + bias score ($s_{j,t} + b_j$)\n",
        "    * This effectively activates only the top-K experts for each token, masking out the rest.\n",
        "    * This promote balanced usage, i.e. prevent some experts from being overused while others stay idle.\n",
        "\n",
        "4. **Final Output**:\n",
        "* $$O_t = X_t + O_s + O_r $$\n",
        "\n",
        "$\\quad$\n",
        "5. **Update Bias term**\n",
        "* Count how many times a particular expert $i$ is selected:\n",
        "  * $$ C_i = \\text{count}(i \\in \\text{topk_indices}), \\quad i = 1, \\dots, N_r$$\n",
        "\n",
        "* Calculate the actual load proportion of expert $i$:\n",
        "  * $$ L_i = \\frac{C_i}{T}, $$\n",
        "    * where $T$ is the total number of tokens (Batch_size $\\times$ sequence_length)\n",
        "    * This is the actual number of times expert $i$ is selected\n",
        "\n",
        "* Calculate the ideal (target) load of expert $i$ for balanced routing:\n",
        "  * $$L_i^{target} = \\frac{1}{N_r}$$\n",
        "    * This is the ideal number of times we want expert $i$ to be selected\n",
        "\n",
        "* Load Difference\n",
        "  * $$\\Delta L_i = L_i - L_i^{target} $$\n",
        "    * Positive = overloaded, Negative = underloaded\n",
        "\n",
        "* Update the bias term:\n",
        " * $$b_i \\leftarrow b_i - \\gamma \\Delta L_i $$\n",
        "    * where $\\gamma$ is a hyper-parameter called bias update speed.\n",
        "\n",
        "$\\quad$\n",
        "6. **Complementary Sequence-Wise Auxiliary Loss**\n",
        "\n",
        "* Averages the normalized affinity of expert $i$ across all tokens:\n",
        "  * $$P_i = \\frac{1}{T} \\sum_{t=1}^{T} s'_{i,t}$$\n",
        "\n",
        "* Computes how strongly expert $i$ matches token $t$ relative to others:\n",
        "  * $$s'_{i,t} = \\frac{s_{i,t}}{\\sum_{j=1}^{N_r} s_{j,t}}$$\n",
        "\n",
        "* Measures how often expert $i$ is selected (normalized across tokens):\n",
        "  * $$f_i = \\frac{N_r}{K_r \\, T} \\sum_{t=1}^{T} \\mathbf{1}\\Big(s_{i,t} \\in \\text{TopK}(\\{s_{j,t} \\mid 1 \\le j \\le N_r\\}, K_r)\\Big)$$\n",
        "\n",
        "* Balances expert utilization across tokens:\n",
        "  * $$\\mathcal{L}_{\\text{Bal}} = \\alpha \\sum_{i=1}^{N_r} f_i P_i$$\n",
        "    * where $\\alpha$ is the hyper-parameter for balance factor\n",
        "\n",
        "___\n",
        "Reference: [DeepSeek-V3 Technical Report\n",
        "](https://arxiv.org/html/2412.19437) (Section 2.1.2 DeepSeekMoE with Auxiliary-Loss-Free Load Balancing).\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xHEe8kkUe8Y"
      },
      "source": [
        "Below is the `DeepSeek_MoE` code implementing the mathematics we have just discussed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s1pzbSzPh5T"
      },
      "outputs": [],
      "source": [
        "class DeepSeek_MoE(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size # Embedding dimension\n",
        "        self.latent_dim = config.latent_dim\n",
        "        self.num_shared_experts = config.num_shared_experts\n",
        "        self.num_routed_experts = config.num_routed_experts\n",
        "        self.top_k = config.top_k  # Kr\n",
        "        self.bias_update_speed = config.bias_update_speed\n",
        "        self.balance_alpha = config.balance_alpha\n",
        "\n",
        "        assert self.top_k <= self.num_routed_experts, f\"top_k: ({self.top_k}) exceeds available experts: ({self.num_routed_experts})\"\n",
        "\n",
        "        # Expert centroids for affinity scores\n",
        "        self.expert_centroids = nn.Parameter(\n",
        "            torch.empty(self.num_routed_experts, self.hidden_size)\n",
        "        )\n",
        "\n",
        "        # Bias terms for load balancing\n",
        "        self.register_buffer(\"expert_biases\", torch.zeros(self.num_routed_experts))\n",
        "\n",
        "        # Shared experts\n",
        "        self.shared_experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.hidden_size, self.latent_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(self.latent_dim, self.hidden_size)\n",
        "            ) for _ in range(self.num_shared_experts)\n",
        "        ])\n",
        "\n",
        "        # Routed experts\n",
        "        self.routed_experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(self.hidden_size, self.latent_dim),\n",
        "                nn.SiLU(),\n",
        "                nn.Linear(self.latent_dim, self.hidden_size)\n",
        "            ) for _ in range(self.num_routed_experts)\n",
        "        ])\n",
        "\n",
        "        # Initialize centroids\n",
        "        nn.init.xavier_uniform_(self.expert_centroids)\n",
        "\n",
        "    def forward(self, hidden_states, training=True):\n",
        "\n",
        "        batch_size, seq_len, hidden_dim = hidden_states.shape\n",
        "        assert hidden_dim == self.hidden_size, f\"Input hidden size mismatch: got {hidden_dim}, expected {self.hidden_size}.\"\n",
        "        total_tokens = batch_size * seq_len\n",
        "\n",
        "        # ========== Compute affinity scores ==========\n",
        "        # Equation: s_i,t = Sigmoid(u_t^T e_i)\n",
        "        flat_input = hidden_states.view(-1, hidden_dim)\n",
        "        affinity_scores = torch.sigmoid(\n",
        "            F.linear(flat_input, self.expert_centroids)  # u_t^T e_i\n",
        "        ).view(batch_size, seq_len, self.num_routed_experts)\n",
        "\n",
        "        # ========== Top-K routing with bias ==========\n",
        "        # Equation: Use biased scores s_i,t + b_i for routing selection\n",
        "        biased_scores = affinity_scores + self.expert_biases\n",
        "\n",
        "        # Get top-K experts using biased scores\n",
        "        topk_values, topk_indices = torch.topk(biased_scores, self.top_k, dim=-1)\n",
        "\n",
        "        # Create mask for selected experts\n",
        "        expert_mask = torch.zeros_like(affinity_scores)\n",
        "        expert_mask.scatter_(-1, topk_indices, 1.0)\n",
        "\n",
        "        # ========== Compute gating values ==========\n",
        "        # Equation: g'_i,t = s_i,t if selected, 0 otherwise\n",
        "        selected_scores = affinity_scores * expert_mask\n",
        "\n",
        "        # Equation: g_i,t = g'_i,t / sum_j(g'_j,t) - normalization\n",
        "        gating_values = selected_scores / (selected_scores.sum(dim=-1, keepdim=True) + 1e-8)\n",
        "\n",
        "        # ========== Shared experts computation ==========\n",
        "        # Equation: ∑_{i=1}^{N_s} FFN_i^{(s)}(u_t)\n",
        "        shared_output = sum(expert(hidden_states) for expert in self.shared_experts)\n",
        "\n",
        "        # ========== Routed experts computation ==========\n",
        "        # Equation: ∑_{i=1}^{N_r} g_i,t FFN_i^{(r)}(u_t)\n",
        "        flat_gating = gating_values.view(-1, self.num_routed_experts)\n",
        "        flat_indices = topk_indices.view(-1, self.top_k)\n",
        "\n",
        "        # Precompute all expert outputs: FFN_i^{(r)}(u_t) for all experts\n",
        "        all_expert_outputs = torch.stack([\n",
        "            expert(flat_input) for expert in self.routed_experts\n",
        "        ], dim=1)  # [total_tokens, num_routed_experts, hidden_size]\n",
        "\n",
        "        # Gather outputs for selected experts and apply gating\n",
        "        expanded_indices = flat_indices.unsqueeze(-1).expand(-1, -1, hidden_dim)\n",
        "        selected_outputs = all_expert_outputs.gather(1, expanded_indices)  # Get FFN outputs for top-k experts\n",
        "\n",
        "        gating_weights = flat_gating.gather(1, flat_indices).unsqueeze(-1)  # Get g_i,t for selected experts\n",
        "        routed_output_flat = (selected_outputs * gating_weights).sum(dim=1)  # ∑ g_i,t * FFN_i^{(r)}(u_t)\n",
        "        routed_output = routed_output_flat.view(batch_size, seq_len, hidden_dim)\n",
        "\n",
        "        # ========== Load balancing updates ==========\n",
        "        aux_loss = torch.tensor(0.0, device=hidden_states.device)\n",
        "\n",
        "        if training:\n",
        "            # ========== Bias Update ==========\n",
        "            # Count how many times each expert is selected (or the number of tokens routed to that expert)\n",
        "            expert_counts = torch.bincount(\n",
        "                topk_indices.view(-1),\n",
        "                minlength=self.num_routed_experts\n",
        "            ).float()\n",
        "            expert_loads = expert_counts / total_tokens  # Load proportion for each expert\n",
        "\n",
        "            target_load = torch.ones_like(expert_loads) / self.num_routed_experts  # Ideal balanced load\n",
        "            load_diff = expert_loads - target_load  # Positive = overloaded, Negative = underloaded\n",
        "            # Update: decrease bias for overloaded experts, increase for underloaded\n",
        "            self.expert_biases -= self.bias_update_speed * load_diff\n",
        "\n",
        "            # ========== Sequence-wise Auxiliary Loss ==========\n",
        "            # Equation: f_i = (N_r / (K_r * T)) * ∑_t 𝟙(s_i,t ∈ TopK)\n",
        "            f_i = expert_mask.view(-1, self.num_routed_experts).sum(dim=0)  # Count selections per expert\n",
        "            f_i = f_i * (self.num_routed_experts / (self.top_k * seq_len))  # Normalize by sequence length\n",
        "            f_i = f_i / batch_size  # Average over batch\n",
        "\n",
        "            # Equation: P_i = (1/T) ∑_t s'_i,t where s'_i,t = s_i,t / ∑_j s_j,t\n",
        "            s_prime = affinity_scores / (affinity_scores.sum(dim=-1, keepdim=True) + 1e-8)  # Normalized affinities\n",
        "            P_i = s_prime.view(-1, self.num_routed_experts).mean(dim=0)  # Average over all tokens\n",
        "\n",
        "            # Equation: ℒ_Bal = α * ∑_{i=1}^{N_r} f_i * P_i\n",
        "            aux_loss = self.balance_alpha * (f_i * P_i).sum()\n",
        "\n",
        "        # ========== Final output ==========\n",
        "        # Equation: O_t = X_t + shared_experts +  routed_experts\n",
        "        output = hidden_states + shared_output + routed_output\n",
        "\n",
        "        return output, aux_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws3JgPcMTyrn"
      },
      "source": [
        "The above `DeepSeek_MoE` code can be used as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzJdgPCPfXw6",
        "outputId": "f38ced01-8ca5-446a-996b-e37ad70de5d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Train pass (training = True) ---\n",
            "Input shape:   torch.Size([1, 43, 4])\n",
            "Output shape:  torch.Size([1, 43, 4])\n",
            "Aux loss:      0.009999999776482582\n",
            "\n",
            "--- Inference pass (training = False) ---\n",
            "Input shape:   torch.Size([1, 43, 4])\n",
            "Output shape:  torch.Size([1, 43, 4])\n",
            "Aux loss:      0.0\n",
            "\n",
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek_MoE\n",
        "\n",
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 2\n",
        "    top_k = 2                            # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Initialize DeepSeek_MoE module with the above config\n",
        "moe = DeepSeek_MoE(config).to(device)\n",
        "\n",
        "# Run for training = True\n",
        "print(\"\\n--- Train pass (training = True) ---\")\n",
        "output, aux_loss = moe(embeddings, training=True)\n",
        "\n",
        "print(\"Input shape:  \", embeddings.shape)\n",
        "print(\"Output shape: \", output.shape)\n",
        "print(\"Aux loss:     \", aux_loss.item())\n",
        "\n",
        "# Run for training = False\n",
        "print(\"\\n--- Inference pass (training = False) ---\")\n",
        "output, aux_loss = moe(embeddings, training=False)\n",
        "print(\"Input shape:  \", embeddings.shape)\n",
        "print(\"Output shape: \", output.shape)\n",
        "print(\"Aux loss:     \", aux_loss.item())\n",
        "print(f\"\\nRun successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VRuTF9Zpafe"
      },
      "source": [
        "# Step 7: Multi-Token Prediction (MTP)\n",
        "\n",
        "As the name implies, instead of predicting one token (word) at a time, we predict *D* additional future tokens at each position — hence the name **Multi-Token Prediction (MTP)**.\n",
        "\n",
        "In a single-token prediction setup, there is only one model (Main Model) that predicts tokens one by one (autoregressive manner). In MTP, we have *D* additional modules — **MTP Module 1, MTP Module 2, …, MTP Module D** — each responsible for predicting one more future token beyond what the main model predicts.\n",
        "\n",
        "Below figure shows the overall structure (see explanation below):\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://arxiv.org/html/2412.19437v1/x3.png\" width=\"600\"><br>\n",
        "  <b>Figure Source:</b> Figure 3 from\n",
        "  <a href=\"https://arxiv.org/html/2412.19437v1#S2.F3\" target=\"_blank\">\n",
        "    DeepSeek-V3 Technical Report\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "\n",
        "### Steps\n",
        "\n",
        "1. **Main model**  \n",
        "   Pass all input tokens through the shared embedding layer, $\\mathrm{Emb}(\\cdot)$, and the main Transformer block to get the hidden representations $\\mathbf{h}_i^{(0)}$.  \n",
        "   These representations are used by:  \n",
        "   1. The shared output head (a small linear model), $\\mathrm{OutHead}(\\cdot)$, to predict the *next token* (as usual), then Cross-Entropy loss, and\n",
        "   2. The first MTP module (Module 1) to begin multi-token prediction.\n",
        "\n",
        "2. **First MTP module**  \n",
        "   For each token position $i$, take its representation from the previous depth $\\mathbf{h}_i^{(0)}$ (the main model output) and the embedding of the future token $\\mathrm{Emb}(t_{i+1})$. Normalize both (via RMSNorm), concatenate them, and pass through the linear projection $M_1$, then through the Transformer block $\\mathrm{TRM}_1(\\cdot)$. The resulting hidden $\\mathbf{h}_i^{(1)}$ is then:  \n",
        "   1. Fed to the shared output head $\\mathrm{OutHead}(\\cdot)$ to predict the second token, then Cross-Entropy loss, and  \n",
        "   2. Passed to the next MTP module (Module 2), see above figure.\n",
        "\n",
        "3. **Subsequent MTP modules**  \n",
        "   Repeat the same process for all $D$ MTP modules, each one predicting an additional token ahead while maintaining the causal chain. And then average the Cross-Entropy of all the modules.\n",
        "\n",
        "\n",
        "### Notes\n",
        "\n",
        "- The **embedding layer** $\\mathrm{Emb}(\\cdot)$ and **output head** $\\mathrm{OutHead}(\\cdot)$ are *shared* among the main model and all MTP modules.  \n",
        "- Each module (main + MTP) has its *own* Transformer block $\\mathrm{TRM}_k(\\cdot)$, but they all share the same architecture.  \n",
        "___\n",
        "\n",
        "Here are the equations of MTP:\n",
        "\n",
        "\n",
        "* $$\\mathbf{h}'^{(k)}_i = M_k \\left[ \\mathrm{RMSNorm}\\left(\\mathbf{h}^{(k-1)}_i\\right); \\mathrm{RMSNorm}\\left(\\mathrm{Emb}(t_{i+k})\\right) \\right]$$\n",
        "$\\quad$\n",
        "* $$\\mathbf{h}^{(k)}_{1:T-k} = \\mathrm{TRM}_k\\left(\\mathbf{h}'^{(k)}_{1:T-k}\\right)$$\n",
        "$\\quad$\n",
        "* $$P^{(k)}_{i+k+1} = \\mathrm{OutHead}\\left(\\mathbf{h}^{(k)}_i\\right)$$\n",
        "$\\quad$\n",
        "* $$\\mathcal{L}^{(k)}_{\\mathrm{MTP}} = \\mathrm{CrossEntropy}\\left(P^{(k)}_{2+k:T+1},\\, t_{2+k:T+1}\\right)\n",
        "= -\\frac{1}{T} \\sum_{i=2+k}^{T+1} \\log P^{(k)}_i[t_i]$$\n",
        "$\\quad$\n",
        "* $$\\mathcal{L}_{\\mathrm{MTP}} = \\lambda \\frac{1}{D} \\sum_{k=1}^{D} \\mathcal{L}^{(k)}_{\\mathrm{MTP}}$$\n",
        "\n",
        "  * where\n",
        "    - $k$: MTP module index ($1 \\le k \\ge D$)\n",
        "    - $i$: current token index\n",
        "    - $T$: total number of tokens\n",
        "    - $t_i$: ground-truth token at position $i$\n",
        "    - $i=2+k$: start of valid target range:\n",
        "      - The term $i=2+k$ means that the loss for the MTP module starts from the first position where enough past tokens exist to predict $k$ steps ahead.\n",
        "      - In other words, for the first few tokens, the model does not have enough context to predict $k$ future tokens — so those positions are skipped, and loss computation begins at $i=2+k$.\n",
        "    - $T+1$: end index of shifted target sequence, often the final/eos (end of sentence like full stop) token\n",
        "    - $\\lambda$: weighting factor for total MTP loss\n",
        "\n",
        "\n",
        "Each MTP module k predicts future tokens — tokens that come after the current one.\n",
        "\n",
        "So:\n",
        "- The main model ($k=0$) predicts the next token $t_{i+1}$\n",
        "- The first MTP module ($k=1$) predicts $t_{i+2}$\n",
        "- The second MTP module ($k=2$) predicts $t_{i+3}$\n",
        "- In general:\n",
        "  - Module ($k$) predicts $t_{i+k+1}$\n",
        "  - That means for token position $t_{i}$, the prediction target lies $K+1$ steps ahead.\n",
        "\n",
        "___\n",
        "Reference: [DeepSeek-V3 Technical Report\n",
        "](https://arxiv.org/html/2412.19437) (2.2 Multi-Token Prediction).\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFvxLZyjkEF1"
      },
      "source": [
        "Enough theory, let's code. The below is the `DeepSeek_MTP` code implementing the mathematics we have just discussed. It also used `attention_mask` (ignore future and padded tokens)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zt2zTkFnkoMl"
      },
      "outputs": [],
      "source": [
        "class DeepSeek_MTP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.hidden_size = config.hidden_size # Embedding dimension\n",
        "        self.vocab_size = config.vocab_size\n",
        "        self.num_depths = config.num_depths    # D (Please note that this D is different from Embedding dimension D; feel free to replace it with another notation)\n",
        "        self.lambda_mtp = config.lambda_mtp    # λ\n",
        "        self.max_seq_len = getattr(config, 'max_seq_len', 512)  # Add max sequence length\n",
        "        self.pad_token_id = getattr(config, 'pad_token_id', 50256)  # Default pad token ID to 50256 of tiktoken’s GPT-2 vocab, same as <|endoftext|> in the tiktoken’s GPT-2 vocab\n",
        "\n",
        "        assert self.hidden_size % config.num_heads == 0,f\"hidden_size ({self.hidden_size}) must be divisible by num_heads ({config.num_heads})\"\n",
        "\n",
        "        # ===== Shared layers =====\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)   # shared Emb(·)\n",
        "        self.output_head = nn.Linear(self.hidden_size, self.vocab_size)    # shared OutHead(·)\n",
        "\n",
        "        # ---- Create D Transformer blocks TRM_k ----\n",
        "        self.trm_blocks = nn.ModuleList([\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=self.hidden_size,\n",
        "                nhead=config.num_heads,\n",
        "                dim_feedforward=config.latent_dim,\n",
        "                activation=\"gelu\",\n",
        "                batch_first=True,\n",
        "            )\n",
        "            for _ in range(self.num_depths)\n",
        "        ])\n",
        "\n",
        "        # ---- Projection matrices M_k ∈ ℝ^{d×2d} ----\n",
        "        self.proj_matrices = nn.ParameterList([\n",
        "            nn.Parameter(torch.randn(self.hidden_size, 2 * self.hidden_size))\n",
        "            for _ in range(self.num_depths)\n",
        "        ])\n",
        "\n",
        "        # ---- RMSNorm layers ----\n",
        "        self.rmsnorm_h = nn.RMSNorm(self.hidden_size)\n",
        "        self.rmsnorm_e = nn.RMSNorm(self.hidden_size)\n",
        "\n",
        "        # ---- Precomputed causal mask ----\n",
        "        # Create upper triangular mask with ones above diagonal and convert to boolean\n",
        "        #self.register_buffer(\"causal_mask\", torch.triu(torch.ones(self.max_seq_len, self.max_seq_len), diagonal=1).bool())\n",
        "        self.register_buffer(\"causal_mask\", torch.triu(torch.ones(self.max_seq_len, self.max_seq_len, device=device)).bool())\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states, input_tokens=None, mode=\"train\", attention_mask=True):\n",
        "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
        "\n",
        "        assert hidden_size == self.hidden_size, f\"hidden_states last dim {hidden_size} != expected hidden_size {self.hidden_size}\"\n",
        "\n",
        "        if mode == \"train\":\n",
        "            assert input_tokens is not None, \"input_tokens required in training mode\"\n",
        "            assert input_tokens.shape == (batch_size, seq_len), f\"input_tokens {(input_tokens.shape)} must match batch & seq length of hidden_states= {[batch_size, seq_len]}\"\n",
        "\n",
        "            mtp_losses = []\n",
        "\n",
        "            # Use separate variable to prevent in-place overwriting\n",
        "            h_current = hidden_states\n",
        "\n",
        "            # ===== MTP depths loop =====\n",
        "            for k in range(1, self.num_depths + 1):\n",
        "                current_seq_len = h_current.shape[1]  # Use current sequence length\n",
        "                if current_seq_len - k <= 0:\n",
        "                    break  # nothing left to predict\n",
        "\n",
        "\n",
        "                # ---- h'_i^k = M_k [RMSNorm(h_i^{k−1}); RMSNorm(Emb(t_{i+k}))] ----\n",
        "                h_prev = h_current[:, :current_seq_len - k, :]            # h_i^{k−1}\n",
        "                emb_shifted = self.embedding(input_tokens[:, k:])         # Emb(t_{i+k})\n",
        "                h_prev_norm = self.rmsnorm_h(h_prev)\n",
        "                emb_norm = self.rmsnorm_e(emb_shifted)\n",
        "                concat = torch.cat([h_prev_norm, emb_norm], dim=-1)       # concat [h; e]\n",
        "                h_prime_k = torch.matmul(concat, self.proj_matrices[k - 1].T)\n",
        "\n",
        "                # ---- causal + padding attention mask ----\n",
        "                causal_mask = None\n",
        "                padding_mask = None\n",
        "\n",
        "                if attention_mask:\n",
        "                    # Get the actual sequence length for this depth\n",
        "                    L = current_seq_len - k\n",
        "\n",
        "                    # Original mask truncated to the number of tokens and converted to boolean\n",
        "                    causal_mask = self.causal_mask[:L, :L]\n",
        "\n",
        "                    # Create padding mask from input tokens (also boolean)\n",
        "                    padding_mask = (input_tokens[:, k:current_seq_len] == self.pad_token_id)  # (B, L)\n",
        "\n",
        "                # ---- Transformer block TRM_k(h'_i^k) ----\n",
        "                h_k = self.trm_blocks[k - 1](h_prime_k, src_mask=causal_mask, src_key_padding_mask=padding_mask)\n",
        "\n",
        "                # ---- logits = OutHead(h_i^k) ----\n",
        "                mtp_logits = self.output_head(h_k)\n",
        "\n",
        "                # ---- Cross-entropy loss ----\n",
        "                target_k = input_tokens[:, k:current_seq_len]  # shift targets by +k, match current length\n",
        "                loss_k = F.cross_entropy(\n",
        "                    mtp_logits.reshape(-1, self.vocab_size),\n",
        "                    target_k.reshape(-1),\n",
        "                    reduction=\"mean\",\n",
        "                    ignore_index=self.pad_token_id\n",
        "                )\n",
        "                mtp_losses.append(loss_k)\n",
        "\n",
        "                # Update h_current for next depth (maintain causal chain)\n",
        "                h_current = torch.cat([h_k, h_current[:, current_seq_len - k:, :]], dim=1)\n",
        "\n",
        "            assert mtp_losses, \"No valid MTP losses computed\"\n",
        "            mtp_loss = self.lambda_mtp * torch.stack(mtp_losses).mean()\n",
        "            return mtp_loss, mtp_logits\n",
        "\n",
        "        elif mode == \"inference\":\n",
        "            # completely skip MTP path — just run the shared output head\n",
        "            logits = self.output_head(hidden_states)     # [B, S, V]\n",
        "            predicted_ids = torch.argmax(logits, dim=-1) # [B, S]\n",
        "            return predicted_ids, logits\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode '{mode}', must be 'train' or 'inference'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Yf_85OWkK_D"
      },
      "source": [
        "The above `DeepSeek_MTP` code can be used as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwysQpcQsuvY",
        "outputId": "2172fe46-88a8-4fa8-edd5-adab7286d748"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Train pass (mode = training) ---\n",
            "MTP Loss: 5.518276214599609\n",
            "Train pass run without any error. . .\n",
            "\n",
            "--- Inference pass (mode = inference) ---\n",
            "  • Pos   0 | Input word: I               → Predicted word: �\n",
            "  • Pos   1 | Input word:  love           → Predicted word:  Franc\n",
            "  • Pos   2 | Input word:  AI             → Predicted word: �\n",
            "  • Pos   3 | Input word:  so             → Predicted word:  increasingly\n",
            "  • Pos   4 | Input word:  much           → Predicted word: like\n",
            "  • Pos   5 | Input word: .               → Predicted word:  each\n",
            "  • Pos   6 | Input word:  I              → Predicted word:  Franc\n",
            "  • Pos   7 | Input word:  enjoy          → Predicted word:  Sov\n",
            "  • Pos   8 | Input word:  learning       → Predicted word:  Factors\n",
            "  • Pos   9 | Input word:  about          → Predicted word: yn\n",
            "  • Pos  10 | Input word:  it             → Predicted word: Thomas\n",
            "  • Pos  11 | Input word: ,               → Predicted word:  Lead\n",
            "  • Pos  12 | Input word:  using          → Predicted word:  Franc\n",
            "  • Pos  13 | Input word:  it             → Predicted word: Thomas\n",
            "  • Pos  14 | Input word: ,               → Predicted word:  Lead\n",
            "  • Pos  15 | Input word:  and            → Predicted word: Ser\n",
            "  • Pos  16 | Input word:  seeing         → Predicted word:  contr\n",
            "  • Pos  17 | Input word:  how            → Predicted word:  hurting\n",
            "  • Pos  18 | Input word:  it             → Predicted word: Thomas\n",
            "  • Pos  19 | Input word:  transforms     → Predicted word: ESS\n",
            "\n",
            "\u001b[1mInput sentence:\u001b[0m I love AI so much. I enjoy learning about it, using it, and seeing how it transforms the\n",
            "world around us. Still, we must be ready for an AI bubble, like the dot-com crash.\n",
            "\n",
            "\u001b[1mPredicted sentence:\u001b[0m �  Franc �  increasingly like  each  Franc  Sov  Factors yn Thomas  Lead  Franc Thomas\n",
            "Lead Ser  contr  hurting Thomas ESS\n",
            "\n",
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek_MTP\n",
        "\n",
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 2\n",
        "    top_k = 2                            # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                     # λ, weighting\n",
        "    num_depths = 5                       # D, number of prediction depths\n",
        "    vocab_size= vocab_size               # Vocab size of tiktoken’s GPT-2 vocab (50257), defined earlier\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Initialize DeepSeek_MTP module with the above config\n",
        "mtp_head = DeepSeek_MTP(config).to(device)\n",
        "\n",
        "# Run for training\n",
        "print(\"\\n--- Train pass (mode = training) ---\")\n",
        "mtp_loss, mtp_logits = mtp_head(embeddings, token_ids_tensor, mode = \"train\", attention_mask=True)\n",
        "print(\"MTP Loss:\", mtp_loss.item())\n",
        "print(\"Train pass run without any error. . .\")\n",
        "\n",
        "# Run for Inference\n",
        "print(\"\\n--- Inference pass (mode = inference) ---\")\n",
        "\n",
        "mtp_head.eval()\n",
        "with torch.no_grad(): # Disable gradient tracking for test check (no backward/update)\n",
        "    embeddings_inf = mtp_head.embedding(token_ids_tensor).detach().clone()\n",
        "    predicted_ids,_ = mtp_head(embeddings_inf, mode = \"inference\", input_tokens = token_ids_tensor.to(device))\n",
        "\n",
        "max_words = 20  # maximum number of tokens/words to predict\n",
        "predicted_sentence = \"\"\n",
        "for pos, (input_id, pred_id) in enumerate(zip(token_ids_tensor[0], predicted_ids[0])):\n",
        "    if pos >= max_words:\n",
        "        break\n",
        "    input_word = tokenizer.decode([input_id.item()])\n",
        "    predicted_word = tokenizer.decode([pred_id.item()])\n",
        "    predicted_sentence += predicted_word + \" \"  # add space between tokens\n",
        "    print(f\"  • Pos {pos:>3} | Input word: {input_word:<15} → Predicted word: {predicted_word}\")\n",
        "\n",
        "\n",
        "# Decode original input for reference\n",
        "decoded_sentence = tokenizer.decode(token_ids_tensor[0].tolist())\n",
        "import textwrap\n",
        "#decoded_sentence = textwrap.fill(decoded_sentence.strip(), width=100)\n",
        "print(f\"\\n\\033[1mInput sentence:\\033[0m {textwrap.fill(decoded_sentence.strip(), width=90)}\")\n",
        "print(f\"\\n\\033[1mPredicted sentence:\\033[0m {textwrap.fill(predicted_sentence.strip(), width=90)}\")\n",
        "print(\"\\nRun successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mOrPLG833wzr"
      },
      "source": [
        "**Our predicted sentence above is garbage and nonsensical.** That's ok as we haven't trained our model yet. We are just testing each component of our model individually."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jyx2Tzt-1j0C"
      },
      "source": [
        "Phew! We did a lot of work. Now things get easy. We will now just put all the things together.\n",
        "\n",
        "# Step 8a: DeepSeek V3 Block (Single-Block Transformer)\n",
        "\n",
        "The `DeepSeek_V3_Block` is a single tranformer block that takes the input and passes it through several components, such as MLA and MoE, as shown below.\n",
        "\n",
        "$$\n",
        "\\begin{array}{c}\n",
        "\\fbox{Input} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{RMSNorm1}\\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{MLA Block (Attention)} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{Skip Connection (Residual)} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{RMSNorm2} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{MoE Block (Feed-Forward Network)} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{Skip Connection (Residual)} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{RMSNorm3} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{Linear Output (and KV Cache)} \\\\\n",
        "\\end{array}\n",
        "$$\n",
        "$ \\quad $\n",
        "\n",
        "\n",
        "Below figure from DeepSeek V3 paper shows the same overall structure:\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://arxiv.org/html/2412.19437v1/x2.png\" width=\"600\"><br>\n",
        "  <b>Figure Source:</b> Figure 2 from\n",
        "  <a href=\"https://arxiv.org/html/2412.19437v1#S2.F2\" target=\"_blank\">\n",
        "    DeepSeek-V3 Technical Report\n",
        "  </a>\n",
        "</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6wLgTl4xlec"
      },
      "outputs": [],
      "source": [
        "class DeepSeek_V3_Block(nn.Module):\n",
        "    \"\"\"\n",
        "    Single-Block Transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.layer_norm_eps = config.layer_norm_eps # Small epsilon value for numerical stability in layer normalization\n",
        "\n",
        "        # --- Layers ---\n",
        "\n",
        "        # Input normalization\n",
        "        self.rms_norm1 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
        "\n",
        "        # DeepSeek_MLA\n",
        "        self.attention = DeepSeek_MLA(config)\n",
        "\n",
        "        # Post-attention normalization\n",
        "        self.rms_norm2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
        "\n",
        "        # DeepSeek_MoE\n",
        "        self.moe = DeepSeek_MoE(config)\n",
        "\n",
        "        # Final normalization\n",
        "        self.rms_norm3 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
        "\n",
        "        # Linear Output\n",
        "        self.linear_output = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, hidden_states,input_tokens=None, mode=\"train\", use_cache=False, past_key_values=None, attention_mask=False):\n",
        "\n",
        "\n",
        "        assert hidden_states.dim() == 3, (f\"hidden_states must have shape [batch, seq_len, hidden_size], got {hidden_states.shape}.\")\n",
        "        assert hidden_states.size(-1) == self.hidden_size, (f\"Last dim mismatch: expected {self.hidden_size}, got {hidden_states.size(-1)}.\")\n",
        "\n",
        "        # Input normalization\n",
        "        normed_states = self.rms_norm1(hidden_states)\n",
        "\n",
        "        # Multi-Head Latent Attention\n",
        "        attn_output, kv_cache = self.attention(\n",
        "            hidden_states = normed_states,\n",
        "            input_tokens = input_tokens,\n",
        "            mode= mode,\n",
        "            use_cache= use_cache,\n",
        "            past_key_values=past_key_values,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "\n",
        "        assert attn_output.shape == hidden_states.shape, (f\"attn_output shape {attn_output.shape} != hidden_states {hidden_states.shape}.\")\n",
        "\n",
        "        # Residual connection\n",
        "        hidden_states = hidden_states + attn_output\n",
        "\n",
        "        # Post-attention normalization\n",
        "        normed_states = self.rms_norm2(hidden_states)\n",
        "\n",
        "        # DeepSeekMoE\n",
        "        moe_output, aux_loss = self.moe(normed_states)\n",
        "\n",
        "        # Residual connection\n",
        "        hidden_states = hidden_states + moe_output\n",
        "\n",
        "        # Final normalization\n",
        "        hidden_states = self.rms_norm3(hidden_states)\n",
        "\n",
        "        # Final Output\n",
        "        hidden_states = self.linear_output(hidden_states)\n",
        "\n",
        "        return hidden_states, kv_cache, aux_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5kHm9JUix4kI",
        "outputId": "7fcddce7-e476-49d2-ca88-adae3b5a74c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek_V3_Block\n",
        "\n",
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 2\n",
        "    top_k = 2                            # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                     # λ, weighting\n",
        "    num_depths = 5                       # D, number of prediction depths\n",
        "    vocab_size= vocab_size               # Vocab size of tiktoken’s GPT-2 vocab (50257), defined earlier\n",
        "    layer_norm_eps = 1e-5                # Small epsilon value for numerical stability in layer normalization\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Initialize DeepSeek_V3_Block module with the above config\n",
        "Block = DeepSeek_V3_Block(config).to(device)\n",
        "\n",
        "#  Forward pass (Train mode)\n",
        "hidden_states, kv_cache, aux_loss = Block(hidden_states = embeddings,input_tokens=token_ids_tensor, mode=\"train\", past_key_values=None, attention_mask=True)\n",
        "\n",
        "#  Inference mode\n",
        "Block.eval()\n",
        "with torch.no_grad(): # Disable gradient tracking for test check (no backward/update)\n",
        "  hidden_states, kv_cache, aux_loss = Block(hidden_states = embeddings,input_tokens=token_ids_tensor, mode=\"inference\",use_cache=True, past_key_values=kv_cache, attention_mask=True)\n",
        "\n",
        "print(f\"\\nRun successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdGMSEiFCOgz"
      },
      "source": [
        "# Step 8b: DeepSeek V3 Encoder (Multi-Block Transformer)\n",
        "\n",
        "The `DeepSeek_V3_Encoder` stacks multiple `DeepSeek_V3_Block` along with final normalization, output layer, and MTP head to produce transformer outputs and predictions.\n",
        "\n",
        "$$\n",
        "\\begin{array}{c}\n",
        "\\fbox{Input} \\\\\n",
        "\\downarrow \\\\\n",
        "\\underbrace{\n",
        "\\begin{array}{c}\n",
        "\\fbox{DeepSeek_V3_Block} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{DeepSeek_V3_Block} \\\\\n",
        "\\downarrow \\\\\n",
        "\\vdots \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{DeepSeek_V3_Block} \\\\\n",
        "\\end{array}\n",
        "}_{\\text{Repeated `num_blocks` times}} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{Final RMSNorm} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{Linear Output} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{MTP Head (Multi-Token Prediction)} \\\\\n",
        "\\swarrow \\qquad \\searrow \\\\\n",
        "\\begin{array}{cc}\n",
        "\\fbox{Training} & \\fbox{Inference} \\\\\n",
        "\\downarrow & \\downarrow \\\\\n",
        "\\fbox{Logits, Loss} & \\fbox{Logits, Predictions}\n",
        "\\end{array}\n",
        "\\end{array}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANAYHpG_-x9H"
      },
      "outputs": [],
      "source": [
        "class DeepSeek_V3_Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Block Transformer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_blocks = config.num_blocks # Number of transformer blocks to stack in the model\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.layer_norm_eps = config.layer_norm_eps\n",
        "        self.vocab_size = config.vocab_size\n",
        "\n",
        "        # Stack of transformer blocks\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DeepSeek_V3_Block(config)\n",
        "            for _ in range(self.num_blocks)\n",
        "        ])\n",
        "\n",
        "        # Final normalization\n",
        "        self.final_norm = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n",
        "\n",
        "        # ---- Final output ----\n",
        "        self.output = nn.Linear(self.hidden_size, self.vocab_size, bias=False)\n",
        "\n",
        "        # MTP head (Multi-Token Prediction)\n",
        "        self.mtp = DeepSeek_MTP(config)\n",
        "\n",
        "\n",
        "    def forward(self,hidden_states, input_tokens=None, mode=\"train\", past_key_values=None, use_cache=False, attention_mask=False):\n",
        "\n",
        "        assert hidden_states.dim() == 3, (f\"hidden_states must have shape [batch, seq_len, hidden_size], got {hidden_states.shape}.\")\n",
        "\n",
        "        if past_key_values is None:\n",
        "            past_key_values = [None] * self.num_blocks\n",
        "\n",
        "        new_past_key_values = [] if use_cache else None\n",
        "\n",
        "        # Forward through stacked transformer blocks\n",
        "        for i, block in enumerate(self.blocks):\n",
        "            hidden_states, kv_cache, aux_loss = block(\n",
        "                hidden_states=hidden_states,\n",
        "                input_tokens=input_tokens,\n",
        "                mode=mode,\n",
        "                use_cache=use_cache,\n",
        "                past_key_values=past_key_values[i],\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "            if use_cache:\n",
        "              new_past_key_values.append(kv_cache)\n",
        "\n",
        "        # Final normalization\n",
        "        hidden_states = self.final_norm(hidden_states)\n",
        "\n",
        "        # Output\n",
        "        logits = self.output(hidden_states)  # [B, S, V]\n",
        "\n",
        "        # MTP output handling\n",
        "        if mode == \"train\" and input_tokens is not None:\n",
        "            mtp_loss, mtp_logits = self.mtp(\n",
        "                hidden_states=hidden_states,\n",
        "                input_tokens=input_tokens,\n",
        "                mode = \"train\",\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            return logits, mtp_loss, mtp_logits, aux_loss\n",
        "        else: # mode == \"inference\"\n",
        "            predicted_ids, mtp_logits = self.mtp(\n",
        "                hidden_states,\n",
        "                input_tokens=input_tokens,\n",
        "                mode = \"inference\",\n",
        "                attention_mask=attention_mask\n",
        "            )\n",
        "            return predicted_ids, logits, new_past_key_values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU5d91YtNE8M",
        "outputId": "c6c7e758-4aa4-458a-bab6-6343044de2e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train mode\n",
            "MTP loss: 5.5131964683532715\n",
            "Aux loss: 0.009999999776482582\n",
            "Train Logits shape: torch.Size([1, 43, 50257])\n",
            "Train mode passed test.\n",
            "\n",
            "Inference mode\n",
            "Input IDs shape: torch.Size([1, 43])\n",
            "Predicted IDs shape: torch.Size([1, 43])\n",
            "Inference Logits shape: torch.Size([1, 43, 50257])\n",
            "Number of cache layers: 4\n",
            "Inference mode passed test.\n",
            "\n",
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek_V3_Encoder\n",
        "\n",
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 2\n",
        "    top_k = 2                            # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                     # λ, weighting\n",
        "    num_depths = 5                       # D, number of prediction depths\n",
        "    vocab_size= vocab_size               # Vocab size of tiktoken’s GPT-2 vocab (50257), defined earlier\n",
        "    layer_norm_eps = 1e-5                # Small epsilon value for numerical stability in layer normalization\n",
        "    num_blocks = 4                       # Number of transformer blocks to stack in the model\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Initialize DeepSeek_V3_Encoder module with the above config\n",
        "Encode = DeepSeek_V3_Encoder(config).to(device)\n",
        "\n",
        "hidden_states = embeddings.to(device)\n",
        "\n",
        "#  Forward pass (Train mode)\n",
        "Encode.train()\n",
        "with torch.no_grad(): # Disable gradient tracking for test check (no backward/update)\n",
        "    print(\"\\nTrain mode\")\n",
        "    logits, mtp_loss, mtp_logits, aux_loss = Encode(\n",
        "        hidden_states=hidden_states,\n",
        "        input_tokens=token_ids_tensor,\n",
        "        attention_mask=True,\n",
        "        mode=\"train\",\n",
        "        use_cache=False\n",
        "    )\n",
        "    print(\"MTP loss:\", mtp_loss.item())\n",
        "    assert torch.isfinite(mtp_loss).all(), \"MTP Loss contains NaN or Inf.\"\n",
        "    print(\"Aux loss:\", aux_loss.item())\n",
        "    assert torch.isfinite(aux_loss).all(), \"Aux Loss contains NaN or Inf.\"\n",
        "    print(\"Train Logits shape:\", logits.shape)\n",
        "    assert logits.shape[:-1] == hidden_states.shape[:2], \"Logits shape mismatch.\"\n",
        "    print(\"Train mode passed test.\")\n",
        "\n",
        "\n",
        "#  Inference mode\n",
        "Encode.eval()\n",
        "with torch.no_grad(): # Disable gradient tracking for test check (no backward/update)\n",
        "    print(\"\\nInference mode\")\n",
        "    predicted_ids, logits, new_kv_cache = Encode(\n",
        "        hidden_states=hidden_states,\n",
        "        input_tokens=token_ids_tensor,\n",
        "        attention_mask=True,\n",
        "        mode=\"inference\",\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "    print(\"Input IDs shape:\", token_ids_tensor.shape)\n",
        "    print(\"Predicted IDs shape:\", predicted_ids.shape)\n",
        "    print(\"Inference Logits shape:\", logits.shape)\n",
        "    print(\"Number of cache layers:\", len(new_kv_cache))\n",
        "\n",
        "    assert logits.shape[:-1] == hidden_states.shape[:2], \"Logits shape mismatch.\"\n",
        "    assert len(new_kv_cache) == config.num_blocks, \"Number of cache layers mismatch.\"\n",
        "    print(\"Inference mode passed test.\")\n",
        "\n",
        "print(f\"\\nRun successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Qt8Yyeau07"
      },
      "source": [
        "# Step 8c: DeepSeek V3 Model (Full Model)\n",
        "\n",
        "The `DeepSeek_V3_Model` takes token IDs, computes embeddings so we don't need to do it separately, and runs them through the `DeepSeek_V3_Encoder`, handling training loss or inference outputs.\n",
        "\n",
        "**\"[DeepSeek V3 Model] is all you need.\"**\n",
        "\n",
        "$$\n",
        "\\begin{array}{c}\n",
        "\\fbox{Input Tokens} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{Embedding} \\\\\n",
        "\\downarrow \\\\\n",
        "\\fbox{DeepSeek_V3_Encoder} \\\\\n",
        "\\swarrow \\qquad \\searrow \\\\\n",
        "\\begin{array}{cc}\n",
        "\\fbox{Training} & \\fbox{Inference} \\\\\n",
        "\\downarrow & \\downarrow \\\\\n",
        "\\fbox{Loss} & \\fbox{Predictions}\n",
        "\\end{array}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LlZegKNRizt8"
      },
      "outputs": [],
      "source": [
        "class DeepSeek_V3_Model(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = nn.Embedding(config.vocab_size, config.hidden_size)\n",
        "\n",
        "        # Core model\n",
        "        self.model = DeepSeek_V3_Encoder(config)\n",
        "\n",
        "        # Loss functions\n",
        "        self.ce_loss = nn.CrossEntropyLoss(ignore_index=config.pad_token_id)\n",
        "\n",
        "\n",
        "    def forward(self, input_tokens = None, mode=\"train\", use_cache=False, past_key_values=None,attention_mask=False):\n",
        "\n",
        "        # Generate embeddings from input tokens\n",
        "        hidden_states = self.embedding(input_tokens)\n",
        "        batch_size, seq_len = input_tokens.shape\n",
        "\n",
        "        # Core model forward\n",
        "        outputs = self.model(\n",
        "            hidden_states,\n",
        "            mode=mode,\n",
        "            input_tokens=input_tokens,\n",
        "            attention_mask=attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache\n",
        "        )\n",
        "\n",
        "        if mode == \"train\":\n",
        "            logits, mtp_loss, mtp_logits, aux_loss = outputs\n",
        "\n",
        "            # Shift for next-token prediction\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = input_tokens[..., 1:].contiguous()\n",
        "\n",
        "            main_loss = self.ce_loss(\n",
        "                shift_logits.view(-1, self.config.vocab_size),\n",
        "                shift_labels.view(-1)\n",
        "            )\n",
        "\n",
        "            # Combine losses\n",
        "            total_loss = main_loss + mtp_loss\n",
        "            return total_loss, main_loss, mtp_loss, aux_loss, logits\n",
        "\n",
        "        else: # mode == \"inference\"\n",
        "            predicted_ids, logits, new_cache = outputs\n",
        "            return predicted_ids, logits, new_cache\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaJXsQbjJTcy",
        "outputId": "19376fce-ed9a-4465-c501-b172adbf4a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train mode\n",
            "total_loss loss: 16.591777801513672\n",
            "main_loss loss: 11.074323654174805\n",
            "mtp_loss loss: 5.517454624176025\n",
            "aux_loss loss: 0.009999999776482582\n",
            "Train mode passed test.\n",
            "Gradients passed test.\n",
            "\n",
            "Inference mode\n",
            "Predicted IDs shape: torch.Size([1, 43])\n",
            "Inference Logits shape: torch.Size([1, 43, 50257])\n",
            "Number of cache layers: 4\n",
            "Inference mode passed test.\n",
            "\n",
            "\u001b[1mInput sentence:\u001b[0m I love AI so much. I enjoy learning about it, using it, and seeing how it transforms the\n",
            "world around us. Still, we must be ready for an AI bubble, like the dot-com crash.\n",
            "\n",
            "\u001b[1mPredicted sentence:\u001b[0m hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes\n",
            "hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes\n",
            "hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes hopes\n",
            "\n",
            "Run successfully!\n"
          ]
        }
      ],
      "source": [
        "# DeepSeek_V3_Model\n",
        "\n",
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 2\n",
        "    top_k = 2                            # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                     # λ, weighting\n",
        "    num_depths = 5                       # D, number of prediction depths\n",
        "    vocab_size= vocab_size               # Vocab size of tiktoken’s GPT-2 vocab (50257), defined earlier\n",
        "    layer_norm_eps = 1e-5                # Small epsilon value for numerical stability in layer normalization\n",
        "    num_blocks = 4                       # Number of transformer blocks to stack in the model\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Initialize DeepSeek_V3_Model module with the above config\n",
        "Model = DeepSeek_V3_Model(config).to(device)\n",
        "\n",
        "#  Forward pass (Train mode)\n",
        "Model.train()\n",
        "#with torch.no_grad(): # Disable gradient tracking for test check (no backward/update)\n",
        "print(\"\\nTrain mode\")\n",
        "total_loss, main_loss, mtp_loss, aux_loss, logits = Model(\n",
        "    input_tokens = token_ids_tensor,\n",
        "    mode=\"train\",\n",
        "    attention_mask=True,\n",
        "    use_cache=False)\n",
        "\n",
        "print(\"total_loss loss:\", total_loss.item())\n",
        "assert torch.isfinite(total_loss).all(), \"total_loss contains NaN or Inf.\"\n",
        "print(\"main_loss loss:\", main_loss.item())\n",
        "assert torch.isfinite(main_loss).all(), \"main_loss  contains NaN or Inf.\"\n",
        "print(\"mtp_loss loss:\", mtp_loss.item())\n",
        "assert torch.isfinite(mtp_loss).all(), \"MTP Loss contains NaN or Inf.\"\n",
        "print(\"aux_loss loss:\", aux_loss.item())\n",
        "assert torch.isfinite(aux_loss).all(), \"Aux Loss contains NaN or Inf.\"\n",
        "print(\"Train mode passed test.\")\n",
        "\n",
        "# Test that gradients flow\n",
        "total_loss.backward()\n",
        "print(\"Gradients passed test.\")\n",
        "\n",
        "#  Inference mode\n",
        "Model.eval()\n",
        "with torch.no_grad(): # Disable gradient tracking for test check (no backward/update)\n",
        "    print(\"\\nInference mode\")\n",
        "    predicted_ids, logits, new_kv_cache = Model(\n",
        "        input_tokens = token_ids_tensor,\n",
        "        mode=\"inference\",\n",
        "        attention_mask=True,\n",
        "        use_cache=True)\n",
        "\n",
        "    print(\"Predicted IDs shape:\", predicted_ids.shape)\n",
        "    print(\"Inference Logits shape:\", logits.shape)\n",
        "    print(\"Number of cache layers:\", len(new_kv_cache))\n",
        "\n",
        "    assert logits.shape[:-1] == hidden_states.shape[:2], \"Logits shape mismatch.\"\n",
        "    assert len(new_kv_cache) == config.num_blocks, \"Number of cache layers mismatch.\"\n",
        "    print(\"Inference mode passed test.\")\n",
        "\n",
        "decoded_predicted_sentence = tokenizer.decode(predicted_ids[0].tolist())\n",
        "print(f\"\\n\\033[1mInput sentence:\\033[0m {textwrap.fill(decoded_sentence.strip(), width=90)}\")\n",
        "print(f\"\\n\\033[1mPredicted sentence:\\033[0m {textwrap.fill(decoded_predicted_sentence.strip(), width=90)}\")\n",
        "print(\"\\nRun successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_3wL7_LPJ-6"
      },
      "source": [
        "Whew! Almost done. Again, above predicted sentence is garbage as we haven't trained our model yet. We will do this now.\n",
        "\n",
        "But before training, we have to load proper dataset and divide it to train and validation set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQEMRfe_nJS7"
      },
      "source": [
        "####**Data Preparation**\n",
        "\n",
        "The code below connects Colab to your Google Drive and changes the working directory to the `LLM` folder where this notebook is located. You must already have a folder named `LLM` in your Drive (or adjust the path to match your own folder structure). If you are working locally (PC, laptop, etc.) and not using Google Drive, you do not need to run this or mount Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFQJpvHFYjuW",
        "outputId": "85a409f7-f6e8-4a0d-cc2f-ea8266d15b3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#Connect Colab with Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/LLM') # Adjust it accordingly\n",
        "os.path.exists('/content/drive/My Drive/LLM') # Adjust it accordingly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TugxKLS9vTN4"
      },
      "source": [
        "I am using two PDF textbooks:\n",
        "1. Fundamentals of Reservoir Engineering by L.P. Dake\n",
        "2. Reservoir Engineering Handbook by Tarek Ahmed\n",
        "\n",
        "Due to copyright restrictions, I cannot share these books publicly. They are placed in the `Data` folder, which is in the same directory as this notebook (`LLM` folder). If your folder has a different name or location, adjust the file paths in your code accordingly. You should store any books, files, papers, or documents you plan to use for pretraining your model in your chosen directory and load them from there.\n",
        "\n",
        "<pre>\n",
        "My Drive/LLM/\n",
        "├─ Notebook.ipynb\n",
        "└─ Data/\n",
        "  ├─ Fundamentals of Reservoir Engineering.pdf\n",
        "  └─ Reservoir Engineering Handbook.pdf\n",
        "</pre>\n",
        "\n",
        "\n",
        "The code below loads the two books from the `Data` folder:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_gcuGYUYlal",
        "outputId": "300239d7-0b64-4bf5-94e9-e7035871abf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/LLM/Data/Fundamentals of Reservoir Engineering (L.P. Dake).pdf',\n",
              " '/content/drive/My Drive/LLM/Data/Reservoir_Engineering_Handbook_Ahmed_Tar.pdf']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "! pip install -q pymupdf  # PyMuPDF for PDFs\n",
        "#! pip install -q python-docx  # For .docx\n",
        "\n",
        "import fitz  # PyMuPDF for PDFs\n",
        "#from docx import Document  # For .docx\n",
        "\n",
        "def read_file(file_path):\n",
        "    ext = os.path.splitext(file_path)[1].lower()\n",
        "    if ext == \".pdf\":\n",
        "        with fitz.open(file_path) as doc:\n",
        "            return \"\\n\".join(page.get_text() for page in doc)\n",
        "    #elif ext == \".docx\":\n",
        "     #   doc = Document(file_path)\n",
        "      #  return \"\\n\".join(p.text for p in doc.paragraphs)\n",
        "    #elif ext == \".txt\":\n",
        "     #   with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "      #      return f.read()\n",
        "    else:\n",
        "        return f\"[Unsupported format: {ext}]\"\n",
        "\n",
        "# Set path to your data folder\n",
        "data_folder = os.path.join(os.getcwd(), \"Data\")\n",
        "\n",
        "# List all files in the Data folder with full paths\n",
        "all_files = [os.path.join(data_folder, f) for f in os.listdir(data_folder)]\n",
        "all_files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyCAEGEbYoE5"
      },
      "outputs": [],
      "source": [
        "# Read and merge text data\n",
        "merged_text = \"\"\n",
        "for file_path in all_files:\n",
        "    content = read_file(file_path)\n",
        "    merged_text += content.strip() + \"\\n<|endoftext|>\\n\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwzTTCFLUWX_"
      },
      "source": [
        "Below is some statistics of our dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrCGXwslUdIV",
        "outputId": "11289322-8191-4982-bad6-abd184230309"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1mText Statistics:\u001b[0m\n",
            "Total words        : 544779\n",
            "Total characters   : 3036056\n",
            "Unique words       : 58115\n",
            "\n",
            "\u001b[1mText Preview (first 250 words):\u001b[0m\n",
            "Developments in Petroleum Science, 8 fundamentals of reservoir engineering FURTHER TITLES\n",
            "IN THIS SERIES 1 A. GENE COLLINS GEOCHEMISTRY OF OILFIELD WATERS 2 W.H. FERTL ABNORMAL\n",
            "FORMATION PRESSURES 3 A.P. SZILAS PRODUCTION AND TRANSPORT OF OIL AND GAS 4 C.E.B.\n",
            "CONYBEARE GEOMORPHOLOGY OF OIL AND GAS FIELDS IN SANDSTONE BODIES 5 T.F. YEN and G.V.\n",
            "CHILINGARIAN (Editors) OIL SHALE 6 D.W. PEACEMAN FUNDAMENTALS OF NUMERICAL RESERVOIR\n",
            "SIMULATION 7 G.V. CHILINGARIAN and T.F. YEN (Editors) BITUMENS, ASPHALTS AND TAR SANDS 8\n",
            "L.P. DAKE FUNDAMENTALS OF RESERVOIR ENGINEERING 9 K. MAGARA COMPACTION AND FLUID MIGRATION\n",
            "10 M.T. SILVIA and E.A. ROBINSON DECONVOLUTION OF GEOPHYSICAL TIME SERIES IN THE\n",
            "EXPLORATION FOR OIL AND NATURAL GAS Developments in Petroleum Science, 8 fundamentals of\n",
            "reservoir engineering LP. DAKE Senior Lecturer in Reservoir Engineering, Shell\n",
            "Internationale Petroleum Maatschappij B. V., The Hague, The Netherlands ELSEVIER,\n",
            "AmsterdamLondonNew YorkTokyo ELSEVIER SCIENCE B.V. Sara Burgerhartstraat 25 P.O. Box\n",
            "211, 1000 AE Amsterdam, The Netherlands First edition 1978 Second impression l979 Third\n",
            "impression 1980 Fourth impression 1981 Fifth impression 1982 Sixth impression 1982 Seventh\n",
            "impression 1983 Eighth impression 1985 Ninth impression 1986 Tenth impression 1988\n",
            "Eleventh impression 1990 Twelfth impression 1991 Thirteenth impression 1993 Fourteenth\n",
            "impression 1994 Fifteenth impression 1995 Sixteenth impression 1997 Seventeenth impression\n",
            "1998 ISBN 0-444-41830-X  1978 ELSEVIER SCIENCE B.V. All rights reserved. No part of this\n",
            "publication may be reproduced, stored in a retrieval system or transmitted in any form or\n",
            "by any means, electronic, mechanical, photocopying, recording or otherwise, without the\n",
            "prior written permission of\n"
          ]
        }
      ],
      "source": [
        "print(f\"\\n\\033[1mText Statistics:\\033[0m\")\n",
        "\n",
        "# Text Statistics\n",
        "total_words = len(merged_text.split())\n",
        "total_chars = len(merged_text)\n",
        "unique_words = len(set(merged_text.split()))\n",
        "\n",
        "print(f\"Total words        : {total_words}\")\n",
        "print(f\"Total characters   : {total_chars}\")\n",
        "print(f\"Unique words       : {unique_words}\")\n",
        "\n",
        "# Limit to first 250 words\n",
        "limited_text = \" \".join(merged_text.split()[:250])\n",
        "print(f\"\\n\\033[1mText Preview (first 250 words):\\033[0m\")\n",
        "print(textwrap.fill(limited_text, width=90))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIwU7tbHW7kj"
      },
      "source": [
        "Ok, now we have to do two things:\n",
        " 1. Convert text to tokens\n",
        " 2. Split the dataset into training and validation sets.\n",
        "\n",
        "For this, I am borrowing code from [Sebastian Raschka](https://www.linkedin.com/in/sebastianraschka/); his [LLM book](https://www.manning.com/books/build-a-large-language-model-from-scratch), [articles](https://magazine.sebastianraschka.com/), and [GitHub](https://github.com/rasbt/LLMs-from-scratch) are excellent. I highly recommend following him. The code below handles both tokenization and dataset splitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TM7YPLOXXVvW"
      },
      "outputs": [],
      "source": [
        "# Code from  Sebastian Raschka\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = torch.tensor(token_ids[i:i + max_length], device=device)\n",
        "            target_chunk = torch.tensor(token_ids[i + 1:i + max_length + 1], device=device)\n",
        "            self.input_ids.append(input_chunk)\n",
        "            self.target_ids.append(target_chunk)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgzT-tVVqh4w"
      },
      "source": [
        "Alright, now we will use above code to split dataset into `train_loader` (90%) and `val_loader` (10%)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V_bYMiUHVl2q"
      },
      "outputs": [],
      "source": [
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = embeddings.shape[-1]   # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2        # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 2                        # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 4                          # Positional encoding dimension\n",
        "    pad_token_id = 50256                 # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 2\n",
        "    top_k = 2                            # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                     # λ, weighting\n",
        "    num_depths = 5                       # D, number of prediction depths\n",
        "    vocab_size= vocab_size               # Vocab size of tiktoken’s GPT-2 vocab (50257), defined earlier\n",
        "    layer_norm_eps = 1e-5                # Small epsilon value for numerical stability in layer normalization\n",
        "    num_blocks = 4                       # Number of transformer blocks to stack in the model\n",
        "    batch_size = 64                      # Number of sequences per batch\n",
        "    context_length = 60                  # Number of tokens per sequence\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "# Ensure reproducibility\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Split data\n",
        "train_ratio = 0.90\n",
        "split_idx = int(train_ratio * len(merged_text))\n",
        "train_loader = create_dataloader_v1(\n",
        "    merged_text[:split_idx],\n",
        "    batch_size= config.batch_size,\n",
        "    max_length= config.context_length,\n",
        "    stride= config.context_length,\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "val_loader = create_dataloader_v1(\n",
        "    merged_text[split_idx:],\n",
        "    batch_size=config.batch_size,\n",
        "    max_length= config.context_length,\n",
        "    stride= config.context_length,\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__qGmcZ1rCxx"
      },
      "source": [
        "Check some statistics of our training and validation loaders:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cgJ8xIe0ooS",
        "outputId": "be3ba9d2-fbda-47b9-fc45-23be2a5b859d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of batches in train_loader: 256\n",
            "Number of batches in val_loader: 29\n",
            "\n",
            "Batch info:\n",
            "Sequences per batch (batch_size): 64\n",
            "Tokens per sequence (context_length): 60\n",
            "Total tokens per batch (batch_size x context_length): 3840\n"
          ]
        }
      ],
      "source": [
        "# Number of batches\n",
        "print(\"Number of batches in train_loader:\", len(train_loader))\n",
        "print(\"Number of batches in val_loader:\", len(val_loader))\n",
        "\n",
        "# Check emptiness\n",
        "if len(train_loader) == 0:\n",
        "    print(\"\\ntrain_loader is empty! Adjust your dataset or parameters.\")\n",
        "else:\n",
        "    # Inspect one batch\n",
        "    batch = next(iter(train_loader))\n",
        "\n",
        "    input_ids = batch[0]\n",
        "\n",
        "    print(\"\\nBatch info:\")\n",
        "    print(\"Sequences per batch (batch_size):\", input_ids.shape[0])\n",
        "    print(\"Tokens per sequence (context_length):\", input_ids.shape[1])\n",
        "    print(\"Total tokens per batch (batch_size x context_length):\", input_ids.numel())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fyLwqEU7i9t"
      },
      "source": [
        "## Step 9: Pre-Training the Model\n",
        "\n",
        "Below, we write two functions, `train_model` code to train our DeepSeek V3 model and `generate_text` to produce sample outputs during training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JZi86c-yLxAf"
      },
      "outputs": [],
      "source": [
        "# Code adapted from  Sebastian Raschka\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=50, temperature=1.0, top_k=20, eos_id=None, device=None):\n",
        "\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Encode prompt\n",
        "    input_ids = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long, device=device)\n",
        "    generated = input_ids.clone()\n",
        "\n",
        "    past_key_values = None  # cache for inference\n",
        "\n",
        "    for _ in range(max_length):\n",
        "\n",
        "        if past_key_values is None:\n",
        "            idx_cond = generated          # full prompt (first step)\n",
        "        else:\n",
        "            idx_cond = generated[:, -1:]  # only last token\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Use mode=\"inference\" and cache past keys/values\n",
        "            predicted_ids, logits, past_key_values = model(\n",
        "                input_tokens=idx_cond.to(device),\n",
        "                mode=\"inference\",\n",
        "                use_cache=True,\n",
        "                past_key_values=past_key_values,\n",
        "                attention_mask=True\n",
        "            )\n",
        "\n",
        "            logits = logits[:, -1, :]  # last token logits\n",
        "\n",
        "        # Top-k filtering\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\"), device=device), logits)\n",
        "\n",
        "        # Temperature + sampling or greedy\n",
        "        if temperature > 0:\n",
        "            logits = logits / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # Stop if EOS token generated\n",
        "        if eos_id is not None and next_token.item() == eos_id:\n",
        "            break\n",
        "\n",
        "        # Append generated token\n",
        "        generated = torch.cat((generated, next_token.to(device)), dim=1)\n",
        "\n",
        "    # Decode full sequence back to text\n",
        "    return tokenizer.decode(generated[0].tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9m7UllX79JMm"
      },
      "outputs": [],
      "source": [
        "# Code adapted from  Sebastian Raschka\n",
        "\n",
        "def train_model(model, train_loader, val_loader, device=None, optimizer=None,num_epochs=10, print_every=10,\n",
        "                warmup_steps=200, initial_lr=3e-5, min_lr=1e-6, prompt=None):\n",
        "\n",
        "    # Ensure device\n",
        "    if device is None:\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
        "    tokens_seen, global_step = 0, 0\n",
        "\n",
        "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    total_steps = len(train_loader) * num_epochs\n",
        "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss, train_batches = 0.0, 0\n",
        "\n",
        "        for batch_idx, (input_ids, target_ids) in enumerate(train_loader):\n",
        "            global_step += 1\n",
        "            optimizer.zero_grad()\n",
        "            input_ids = input_ids.to(device)\n",
        "            target_ids = target_ids.to(device)\n",
        "\n",
        "            # ---- LR schedule ----\n",
        "            if global_step < warmup_steps:\n",
        "                lr = initial_lr + lr_increment * global_step\n",
        "            else:\n",
        "                progress = (global_step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "                lr = min_lr + 0.5 * (peak_lr - min_lr) * (1 + math.cos(math.pi * progress))\n",
        "            for pg in optimizer.param_groups:\n",
        "                pg[\"lr\"] = lr\n",
        "            track_lrs.append(lr)\n",
        "\n",
        "            # ---- Forward ----\n",
        "            total_loss, main_loss, mtp_loss, aux_loss, logits = model(\n",
        "                input_tokens=input_ids,\n",
        "                attention_mask=True,\n",
        "                mode=\"train\",\n",
        "                use_cache=False\n",
        "            )\n",
        "\n",
        "            # ---- Backward ----\n",
        "            total_loss.backward()\n",
        "            if global_step >= warmup_steps:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            tokens_seen += input_ids.numel()\n",
        "\n",
        "            total_train_loss += total_loss.item()\n",
        "            train_batches += 1\n",
        "\n",
        "        # ---- Epoch-end evaluation ----\n",
        "        avg_epoch_train_loss = total_train_loss / max(1, train_batches)\n",
        "\n",
        "        total_val_loss, val_batches = 0, 0\n",
        "        model.eval()\n",
        "        ce_loss_fn = nn.CrossEntropyLoss(ignore_index=model.config.pad_token_id)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for v_input, v_target in val_loader:\n",
        "                v_input = v_input.to(device)\n",
        "                v_target = v_target.to(device)\n",
        "\n",
        "                # Forward pass in inference mode\n",
        "                predicted_ids, logits, _ = model(input_tokens=v_input, mode=\"inference\", use_cache=False)\n",
        "\n",
        "                # Compute validation loss\n",
        "                # logits: (batch, seq_len, vocab_size)\n",
        "                # v_target: (batch, seq_len)\n",
        "                batch_loss = ce_loss_fn(\n",
        "                    logits.view(-1, model.config.vocab_size),\n",
        "                    v_target.view(-1)\n",
        "                )\n",
        "\n",
        "                total_val_loss += batch_loss.item()\n",
        "                val_batches += 1\n",
        "\n",
        "        avg_epoch_val_loss = total_val_loss / max(1, val_batches)\n",
        "\n",
        "        train_losses.append(avg_epoch_train_loss)\n",
        "        val_losses.append(avg_epoch_val_loss)\n",
        "        track_tokens_seen.append(tokens_seen)\n",
        "\n",
        "         # Step-wise printing\n",
        "        if (epoch+1) % print_every == 0:\n",
        "            print(f\"\\n------------ Epoch {epoch+1} completed ------------\")\n",
        "            print(f\"[Epoch  {epoch+1}/{num_epochs}    Step  {global_step}/{total_steps}] \"\n",
        "            f\"Total Loss  {total_loss.item():.4f}   |   Main Loss   {main_loss.item():.4f}  |  MTP Loss {mtp_loss.item():.4f}   |  Learning Rate   {lr:.6f}\")\n",
        "            print(f\"Train Loss: {avg_epoch_train_loss:.4f},   Val Loss: {avg_epoch_val_loss:.4f}\")\n",
        "\n",
        "            # Generate a sample if prompt is provided\n",
        "            if prompt is not None:\n",
        "                generated = generate_text(\n",
        "                    model, tokenizer, prompt,\n",
        "                    max_length=10,\n",
        "                    temperature=0.7,\n",
        "                    eos_id=model.config.pad_token_id,\n",
        "                    device=device\n",
        "                )\n",
        "                print(f\"Prompt: '{prompt}'\")\n",
        "                print(f\"Generated Text: '{generated}'\")\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen, track_lrs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mye5GspwGKj"
      },
      "outputs": [],
      "source": [
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = 128                # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2    # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 16                   # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 24                     # Positional encoding dimension\n",
        "    pad_token_id = 50256             # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 8\n",
        "    top_k = 8                        # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                 # λ, weighting\n",
        "    num_depths = 3                   # D, number of prediction depths\n",
        "    vocab_size= vocab_size           # Vocab size of tiktoken's GPT-2 vocab (50257), defined earlier\n",
        "    layer_norm_eps = 1e-5            # Small epsilon value for numerical stability in layer normalization\n",
        "    num_blocks = 12                  # Number of transformer blocks to stack in the model\n",
        "    batch_size = 64                  # Number of sequences per batch\n",
        "    context_length = 60              # Number of tokens per sequence\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqMdwos2-EqP",
        "outputId": "d49cb32d-0b31-4132-9a1a-800761b52f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Model parameters: 29,756,561\n",
            "Starting training...\n",
            "\n",
            "\n",
            "------------ Epoch 2 completed ------------\n",
            "[Epoch  2/10    Step  512/2560] Total Loss  5.1023   |   Main Loss   4.3161  |  MTP Loss 0.7862   |  Learning Rate   0.000479\n",
            "Train Loss: 6.1083,   Val Loss: 5.0822\n",
            "Prompt: 'What is water flooding'\n",
            "Generated Text: 'What is water flooding. The reservoir is\n",
            "the pressure of the reservoir'\n",
            "\n",
            "------------ Epoch 4 completed ------------\n",
            "[Epoch  4/10    Step  1024/2560] Total Loss  3.8718   |   Main Loss   3.6595  |  MTP Loss 0.2123   |  Learning Rate   0.000364\n",
            "Train Loss: 4.0475,   Val Loss: 4.6618\n",
            "Prompt: 'What is water flooding'\n",
            "Generated Text: 'What is water flooding\n",
            "in the pressure is the oil in the pressure'\n",
            "\n",
            "------------ Epoch 6 completed ------------\n",
            "[Epoch  6/10    Step  1536/2560] Total Loss  3.6263   |   Main Loss   3.4999  |  MTP Loss 0.1263   |  Learning Rate   0.000199\n",
            "Train Loss: 3.5278,   Val Loss: 4.5426\n",
            "Prompt: 'What is water flooding'\n",
            "Generated Text: 'What is water flooding-\n",
            "ing the data equation in the waterfl'\n",
            "\n",
            "------------ Epoch 8 completed ------------\n",
            "[Epoch  8/10    Step  2048/2560] Total Loss  3.5077   |   Main Loss   3.4070  |  MTP Loss 0.1007   |  Learning Rate   0.000057\n",
            "Train Loss: 3.2987,   Val Loss: 4.4917\n",
            "Prompt: 'What is water flooding'\n",
            "Generated Text: 'What is water flooding-\n",
            "sure. The test between the reservoir flow'\n",
            "\n",
            "------------ Epoch 10 completed ------------\n",
            "[Epoch  10/10    Step  2560/2560] Total Loss  3.1560   |   Main Loss   3.0819  |  MTP Loss 0.0741   |  Learning Rate   0.000001\n",
            "Train Loss: 3.2166,   Val Loss: 4.4818\n",
            "Prompt: 'What is water flooding'\n",
            "Generated Text: 'What is water flooding-\n",
            "lena and the reservoir. In the'\n",
            "Training completed!\n",
            "Final training loss: 3.2166\n",
            "Final validation loss: 4.4818\n"
          ]
        }
      ],
      "source": [
        "# Model Training\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Initialize model\n",
        "model = DeepSeek_V3_Model(config).to(device)\n",
        "\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4,weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "prompt = \"What is water flooding\"\n",
        "\n",
        "# Train the model\n",
        "print(\"Starting training...\\n\")\n",
        "\n",
        "train_losses, val_losses, track_tokens_seen, track_lrs = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    device=device,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=num_epochs,\n",
        "    print_every=2,\n",
        "    warmup_steps=200,\n",
        "    initial_lr=3e-5,\n",
        "    min_lr=1e-6,\n",
        "    prompt = prompt\n",
        ")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4ufIh3kr55B"
      },
      "source": [
        "Ok, we have trained our model on two books for 10 epochs with very small model configurations.  However, in practice, LLMs such as ChatGPT or DeepSeek are trained with much larger model configurations on millions of books and other large-scale internet data. For instance, DeepSeek-V3 were trained for roughly 58 days on 2,048 NVIDIA H800 GPUs for one or less than two epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1VROtbYNQ19"
      },
      "source": [
        "## Step 10: Autoregressive Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eu0Q7rhoHn0d",
        "outputId": "5c9247cb-60a3-4c9c-a9f5-33f2a90a6770"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Text Generation ===\n",
            "\n",
            "Prompt: 'What is Reservoir Engineering?'\n",
            "Generated: 'What is Reservoir Engineering? \u0001 For the reservoir pressure p (t), with a\n",
            "pressure pore volume). In this pressure (p/d) \u0001 The gas compressibility factor is the\n",
            "following example: \u0001 Pressure-log \u0001 '\n",
            "\n",
            "Prompt: 'What is the role of well logging in petroleum engineering?'\n",
            "Generated: 'What is the role of well logging in petroleum engineering? pressure p/d and is\n",
            "usually expressed in the following two. Step 6. Calculate the water saturations of the two\n",
            "expressions at the gas water saturation from the pressure drop of pressure pore. Example\n",
            "14-4 \u0001'\n",
            "\n",
            "Prompt: 'What is enhanced oil recovery and why is it used?'\n",
            "Generated: 'What is enhanced oil recovery and why is it used? equilibrium (f) proposed\n",
            "using equ. (6.9) is (10.5). The method of the aivity equation are a function of the gas in\n",
            "the liquid for a single conditions of equ. In fact units, it is expressed. In the same oil\n",
            "in a well with the water. For the following steps. As a'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate text with the trained model\n",
        "torch.manual_seed(123)\n",
        "\n",
        "print(\"\\n=== Text Generation ===\\n\")\n",
        "\n",
        "# Example 1\n",
        "prompt1 = \"What is Reservoir Engineering?\"\n",
        "generated1 = generate_text(model, tokenizer, prompt1, max_length=50, temperature=0.7, device=device)\n",
        "\n",
        "print(f\"Prompt: '{prompt1}'\")\n",
        "print(textwrap.fill(f\"Generated: '{generated1}'\", width=90))\n",
        "print()\n",
        "\n",
        "# Example 2\n",
        "prompt2 = \"What is the role of well logging in petroleum engineering?\"\n",
        "generated2 = generate_text(model, tokenizer, prompt2, max_length=50, temperature=1, device=device)\n",
        "print(f\"Prompt: '{prompt2}'\")\n",
        "print(textwrap.fill(f\"Generated: '{generated2}'\", width=90))\n",
        "print()\n",
        "\n",
        "# Example 3\n",
        "prompt3 = \"What is enhanced oil recovery and why is it used?\"\n",
        "generated3 = generate_text(model, tokenizer, prompt3, max_length=75,  temperature=1.2,device=device)\n",
        "print(f\"Prompt: '{prompt3}'\")\n",
        "print(textwrap.fill(f\"Generated: '{generated3}'\", width=90))\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnQqI8UZwNU0"
      },
      "source": [
        "Alright, our model predicted text is still not good which is completely expected. After all, the model was trained on only two books and with very small model configurations. However, you have now learned how to train LLM from scratch. Congratulations!\n",
        "\n",
        "If you have a larger dataset and enough GPU memory, try training a moderate model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J39v1NhfbzVY"
      },
      "source": [
        "#### **Plot loss**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "t3aQqnYlX3gr",
        "outputId": "416dc5e6-bba5-43e6-f741-3e14116d43c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Plot Train and Validation loss\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = len(train_losses) # num_epochs\n",
        "epoch_range = np.arange(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(epoch_range, train_losses, label=\"Train Loss\")\n",
        "plt.plot(epoch_range, val_losses, label=\"Val Loss\")\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdN6S70HBZ9U"
      },
      "source": [
        "# Step 11: Fine-Tuning the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of_w7VVz6M3q"
      },
      "source": [
        "Once the model (\"large model\") is trained (pre-trained), we move to the fine-tuning step. Fine-tuning means retraining only a small part of the model (typically the last layers) on a task or domain-specific dataset such as question-answer pairs.\n",
        "\n",
        "First, we save our pre-trained model so we can run the following code independently of the pre-training (above code).\n",
        "\n",
        "There are two ways to save the pre-trained model in the current directory:\n",
        "\n",
        "1. Save only the model weights\n",
        "    * This produces a smaller file and is sufficient for inference or fine-tuning.\n",
        "\n",
        "2. Save the full training checkpoint (model + optimizer + epoch + losses)\n",
        "    * This is only necessary if you want to resume training exactly from where it stopped, including optimizer state and learning-rate scheduling.\n",
        "\n",
        "The code below have both options. Uncomment the one you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgYRLa4QzUxK",
        "outputId": "61d1a3b7-6437-4d77-8b32-2ed5c8f6b523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model state dict saved at: /content/drive/MyDrive/LLM/DeepsSeek_v3_model.pt\n"
          ]
        }
      ],
      "source": [
        "# Save only the model\n",
        "model_save_path = os.path.join(os.getcwd(), \"DeepsSeek_v3_model.pt\")\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model state dict saved at: {model_save_path}\")\n",
        "\n",
        "# Save full checkpoint\n",
        "#checkpoint_save_path = os.path.join(os.getcwd(), \"DeepsSeek_v3_checkpoint.pt\")\n",
        "#checkpoint = {\n",
        " #   'model_state_dict': model.state_dict(),\n",
        "  #  'optimizer_state_dict': optimizer.state_dict(),\n",
        "   # 'epoch': num_epochs,\n",
        "    #'train_losses': train_losses,\n",
        "    #'val_losses': val_losses\n",
        "#}\n",
        "#torch.save(checkpoint, checkpoint_save_path)\n",
        "#print(f\"Full checkpoint saved at: {checkpoint_save_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvVwzslb9yBK"
      },
      "source": [
        "#### **Load the Model for Fine-Tuning**\n",
        "\n",
        "Let's load the pretrained model `DeepsSeek_v3_model.pt` we saved in the previous step. I am naming it **ChatSPE** (**S**ociety of **P**etroleum **E**ngineers) but you are free to name it anything you want.\n",
        "\n",
        "*Alternatively*, you may want to load any other model like GPT-2, but you will need to adjust the configuration accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4vLDsEj7ROX",
        "outputId": "b052b0cd-3858-4c02-af8c-c7bd2e221329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n"
          ]
        }
      ],
      "source": [
        "# Define model configurations\n",
        "class Config:\n",
        "    hidden_size = 128                # Embedding dimension (D)\n",
        "    latent_dim = hidden_size // 2    # Latent dimension, half of D (a random choice)\n",
        "    num_heads = 16                   # Number of attention heads (should divide hidden_size)\n",
        "    pos_dim = 24                     # Positional encoding dimension\n",
        "    pad_token_id = 50256             # Padding token ID (matches <|endoftext|> in GPT-2 vocab)\n",
        "    num_shared_experts = 4\n",
        "    num_routed_experts = 8\n",
        "    top_k = 8                        # Kr, number of experts selected per token\n",
        "    bias_update_speed = 0.01\n",
        "    balance_alpha = 0.01\n",
        "    lambda_mtp = 0.5                 # λ, weighting\n",
        "    num_depths = 3                   # D, number of prediction depths\n",
        "    vocab_size= vocab_size           # Vocab size of tiktoken’s GPT-2 vocab (50257), defined earlier\n",
        "    layer_norm_eps = 1e-5            # Small epsilon value for numerical stability in layer normalization\n",
        "    num_blocks = 12                  # Number of transformer blocks to stack in the model\n",
        "    batch_size = 64                  # Number of sequences per batch\n",
        "    context_length = 60              # Number of tokens per sequence\n",
        "\n",
        "\n",
        "# Initialize configuration (config)\n",
        "config = Config()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_save_path = os.path.join(os.getcwd(), \"DeepsSeek_v3_model.pt\")\n",
        "\n",
        "# Load model only\n",
        "ChatSPE = DeepSeek_V3_Model(config)\n",
        "ChatSPE.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "ChatSPE.to(device)  # ensure model is on the right device\n",
        "ChatSPE.train()     # ready for fine-tuning\n",
        "print(\"Model loaded.\")\n",
        "\n",
        "# Load full checkpoint\n",
        "#checkpoint_save_path = os.path.join(os.getcwd(), \"DeepsSeek_v3_checkpoint.pt\")\n",
        "#model_full = DeepSeek_V3_Model(config)\n",
        "#optimizer_full = torch.optim.AdamW(model_full.parameters(), lr=5e-4, weight_decay=0.1)\n",
        "# Load checkpoint to the correct device\n",
        "#checkpoint_loaded = torch.load(checkpoint_save_path, map_location=device)\n",
        "# Load model and optimizer states\n",
        "#model_full.load_state_dict(checkpoint_loaded['model_state_dict'])\n",
        "#optimizer_full.load_state_dict(checkpoint_loaded['optimizer_state_dict'])\n",
        "# Move model to device after loading\n",
        "#model_full.to(device)\n",
        "#start_epoch = checkpoint_loaded['epoch']\n",
        "#print(f\"Full checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFJhyXW1KzZp"
      },
      "source": [
        "After loading the pre-trained model, we have to do two things before fine-tuning:\n",
        "\n",
        "1. Prepare the task-specific dataset.\n",
        "2. Decide which layers to fine-tune (full or partial).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t82t9-2XLgTr"
      },
      "source": [
        "#### **Prepare the task-specific dataset:**\n",
        "Below are ten short prompts with their corresponding responses (generated by ChatGPT). But you can use any dataset you want. DeepSeek V3 uses 1.5 millions of prompt-response pairs spanning multiple domains for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOB8dxq-Arfq"
      },
      "outputs": [],
      "source": [
        "txts = [\n",
        "    (\"Prompt: What is porosity?\\nResponse:\", \" Porosity is the fraction of a rock’s bulk volume that consists of pore space.<|endoftext|>\"),\n",
        "    (\"Prompt: What does permeability measure?\\nResponse:\", \" Permeability measures a rock’s ability to transmit fluids through connected pores.<|endoftext|>\"),\n",
        "    (\"Prompt: What is Darcy’s law used for?\\nResponse:\", \" Darcy’s law describes fluid flow through porous media.<|endoftext|>\"),\n",
        "    (\"Prompt: What is water saturation?\\nResponse:\", \" Water saturation is the fraction of pore volume occupied by water.<|endoftext|>\"),\n",
        "    (\"Prompt: What is formation volume factor?\\nResponse:\", \" Formation volume factor relates fluid volume at reservoir conditions to surface conditions.<|endoftext|>\"),\n",
        "    (\"Prompt: What is primary recovery?\\nResponse:\", \" Primary recovery is oil production driven by the reservoir’s natural energy.<|endoftext|>\"),\n",
        "    (\"Prompt: What is capillary pressure?\\nResponse:\", \" Capillary pressure is the pressure difference between two immiscible fluids in porous media.<|endoftext|>\"),\n",
        "    (\"Prompt: What is a reservoir?\\nResponse:\", \" A reservoir is a subsurface rock formation that stores hydrocarbons.<|endoftext|>\"),\n",
        "    (\"Prompt: What does API gravity indicate?\\nResponse:\", \" API gravity indicates the density of crude oil relative to water.<|endoftext|>\"),\n",
        "    (\"Prompt: What is enhanced oil recovery?\\nResponse:\", \" Enhanced oil recovery uses external energy or chemicals to increase oil production beyond primary and secondary recovery.<|endoftext|>\")\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6iABQ51LbCj"
      },
      "source": [
        "The below `PromptResponseDataset` code defines a dataset that returns tokenized prompt-response pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgKK8iuCEFrf"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class PromptResponseDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt, response = self.data[idx]\n",
        "\n",
        "        # tokenize separately and concatenate\n",
        "        prompt_ids = tokenizer.encode(prompt, allowed_special={\"<|endoftext|>\"})\n",
        "        response_ids = tokenizer.encode(response, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # combine prompt and response\n",
        "        full_ids = prompt_ids + response_ids\n",
        "        #full_ids = [min(i, 50256) for i in full_ids]  # clamp to vocab range\n",
        "\n",
        "\n",
        "        # return tensor of token IDs and prompt length (for masking during training)\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(full_ids, dtype=torch.long),\n",
        "            \"prompt_len\": len(prompt_ids)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyosrjwCQL1n"
      },
      "source": [
        "The below `collate_fn` code returns input and target tensors ready for training. It pads a batch of tokenized prompt-response pairs, shifts them for language modeling, masks the prompt and padding tokens..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3A6yMgCY0ZR"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, pad_token_id=50256, ignore_index=-100, device=\"cpu\"):\n",
        "\n",
        "    # find max length in batch\n",
        "    max_len = max(len(item[\"input_ids\"]) for item in batch)\n",
        "\n",
        "    input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for item in batch:\n",
        "        ids = item[\"input_ids\"]\n",
        "        prompt_len = item[\"prompt_len\"]\n",
        "\n",
        "        # pad\n",
        "        padded = torch.full((max_len,), pad_token_id, dtype=torch.long)\n",
        "        padded[:len(ids)] = ids\n",
        "\n",
        "        # shift for LM (Language Model) objective\n",
        "        inputs = padded[:-1]\n",
        "        targets = padded[1:].clone()\n",
        "\n",
        "        # mask padding (keep first pad token, mask the rest)\n",
        "        mask_positions = (padded[1:] == pad_token_id).nonzero(as_tuple=True)[0]\n",
        "        if len(mask_positions) > 0:\n",
        "            targets[mask_positions[1:]] = ignore_index\n",
        "\n",
        "        # mask prompt tokens, only response tokens trained\n",
        "        targets[:prompt_len-1] = ignore_index\n",
        "\n",
        "        # Replace -100 with pad_token_id, so ignore_index=config.pad_token_id works without any error\n",
        "        targets[targets == ignore_index] = pad_token_id\n",
        "\n",
        "        input_batch.append(inputs)\n",
        "        target_batch.append(targets)\n",
        "\n",
        "    input_batch = torch.stack(input_batch).to(device)\n",
        "    target_batch = torch.stack(target_batch).to(device)\n",
        "\n",
        "    return input_batch, target_batch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHnidE-6Z2fS"
      },
      "source": [
        "Split data into train and validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NTea7uNEGW-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 7 train, 3 validation\n",
        "train_txts = txts[:7]\n",
        "val_txts = txts[7:]\n",
        "\n",
        "# Load GPT-2 tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "train_dataset = PromptResponseDataset(train_txts,tokenizer)\n",
        "val_dataset = PromptResponseDataset(val_txts,tokenizer)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda x: collate_fn(x, device=device)\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=lambda x: collate_fn(x, device=device)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1g2HAC68bCEV"
      },
      "source": [
        "#### **Decide which layers to fine-tune (full or partial):**\n",
        "\n",
        "The code below is for full fine-tuning. If you prefer partial fine-tuning, I have included that as well, just uncomment the section after this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tthtBUHRDudl",
        "outputId": "419d966e-5cd9-4af6-d8bd-5a9a9716661e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Fine-Tuning...\n",
            "Total trainable parameters: 29,756,561\n",
            "\n",
            "------------ Epoch 2 completed ------------\n",
            "[Epoch  2/10    Step  4/20] Total Loss  5.2656   |   Main Loss   4.8705  |  MTP Loss 0.3950   |  Learning Rate   0.000039\n",
            "Train Loss: 5.3746,   Val Loss: 5.9746\n",
            "Prompt: 'Prompt: What is porosity?'\n",
            "Generated Text: 'Prompt: What is porosity?\n",
            "\u0001\n",
            "For a function of time, the'\n",
            "\n",
            "------------ Epoch 4 completed ------------\n",
            "[Epoch  4/10    Step  8/20] Total Loss  5.1809   |   Main Loss   4.6758  |  MTP Loss 0.5051   |  Learning Rate   0.000049\n",
            "Train Loss: 4.5629,   Val Loss: 5.9415\n",
            "Prompt: 'Prompt: What is porosity?'\n",
            "Generated Text: 'Prompt: What is porosity?\n",
            "\u0001\n",
            "Water flow rate as the reservoir pressure'\n",
            "\n",
            "------------ Epoch 6 completed ------------\n",
            "[Epoch  6/10    Step  12/20] Total Loss  3.3607   |   Main Loss   3.0558  |  MTP Loss 0.3049   |  Learning Rate   0.000058\n",
            "Train Loss: 3.8352,   Val Loss: 5.9844\n",
            "Prompt: 'Prompt: What is porosity?'\n",
            "Generated Text: 'Prompt: What is porosity?\n",
            "\u0001 SPEarlougher’s'\n",
            "\n",
            "------------ Epoch 8 completed ------------\n",
            "[Epoch  8/10    Step  16/20] Total Loss  3.7052   |   Main Loss   3.2545  |  MTP Loss 0.4507   |  Learning Rate   0.000068\n",
            "Train Loss: 3.4725,   Val Loss: 6.0952\n",
            "Prompt: 'Prompt: What is porosity?'\n",
            "Generated Text: 'Prompt: What is porosity?\n",
            "oil efficiency is the volume factor is the fraction'\n",
            "\n",
            "------------ Epoch 10 completed ------------\n",
            "[Epoch  10/10    Step  20/20] Total Loss  2.7865   |   Main Loss   2.4437  |  MTP Loss 0.3429   |  Learning Rate   0.000077\n",
            "Train Loss: 3.0784,   Val Loss: 6.2076\n",
            "Prompt: 'Prompt: What is porosity?'\n",
            "Generated Text: 'Prompt: What is porosity?\n",
            "isms and water saturation is the fraction of a'\n"
          ]
        }
      ],
      "source": [
        "# Full fine-tuning\n",
        "torch.manual_seed(123)\n",
        "\n",
        "print(f\"Full Fine-Tuning...\")\n",
        "for param in ChatSPE.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "trainable_params = [p for p in ChatSPE.parameters() if p.requires_grad]\n",
        "total_trainable = sum(p.numel() for p in trainable_params)\n",
        "print(f\"Total trainable parameters: {total_trainable:,}\")\n",
        "\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=5e-4, weight_decay=0.1)\n",
        "\n",
        "train_losses, val_losses, _, _ = train_model(\n",
        "    ChatSPE,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device=device,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=10,\n",
        "    print_every=2,\n",
        "    prompt=\"Prompt: What is porosity?\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "2l-XZ__Ncud2",
        "outputId": "d2aa36c0-27bb-4d8e-99c7-6b89792eeac7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntorch.manual_seed(123)\\n\\n# Fine-tune the last transformer block + output layer\\n\\n# Freeze all parameters first\\nfor param in ChatSPE.parameters():\\n    param.requires_grad = False\\n\\n# Find last block\\nblock_indices = set()\\nfor name, _ in ChatSPE.named_parameters():\\n    if \".blocks.\" in name:\\n        # extract index between \".blocks.\" and next \".\"\\n        idx_str = name.split(\".blocks.\")[1].split(\".\")[0]\\n        block_indices.add(int(idx_str))\\n\\nif not block_indices:\\n    raise ValueError(\"No transformer blocks found in model!\")\\n\\n# Last transformer block\\nlast_block_idx = max(block_indices)\\n\\n# Unfreeze last block\\nfor name, param in ChatSPE.named_parameters():\\n    if f\".blocks.{last_block_idx}.\" in name:\\n        param.requires_grad = True\\n\\n# Unfreeze output layers\\nfor name, param in ChatSPE.named_parameters():\\n    if name.startswith(\"model.output\") or name.startswith(\"model.mtp.output_head\"):\\n        param.requires_grad = True\\n\\n\\ntrainable_params = [p for p in ChatSPE.parameters() if p.requires_grad]\\ntotal_trainable = sum(p.numel() for p in trainable_params)\\n\\noptimizer = torch.optim.AdamW(trainable_params, lr=5e-4, weight_decay=0.1)\\n\\nprint(f\"Partial Fine-Tuning...\")\\nprint(f\"Training Last transformer block ({last_block_idx}) + output layer\")\\nprint(f\"Total trainable parameters: {total_trainable:,}\")\\n\\n# Partial Fine-tune\\ntrain_losses, val_losses, _, _ = train_model(\\n    ChatSPE,\\n    train_loader,\\n    val_loader,\\n    device=device,\\n    optimizer=optimizer,\\n    num_epochs=10,\\n    print_every=2,\\n    prompt=\"Prompt: What is porosity?\"\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "# Partial fine-tuning (uncomment to use): Remove \"\"\" and \"\"\" at the start and at the end to uncomment.\n",
        "\"\"\"\n",
        "torch.manual_seed(123)\n",
        "\n",
        "# Fine-tune the last transformer block + output layer\n",
        "\n",
        "# Freeze all parameters first\n",
        "for param in ChatSPE.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Find last block\n",
        "block_indices = set()\n",
        "for name, _ in ChatSPE.named_parameters():\n",
        "    if \".blocks.\" in name:\n",
        "        # extract index between \".blocks.\" and next \".\"\n",
        "        idx_str = name.split(\".blocks.\")[1].split(\".\")[0]\n",
        "        block_indices.add(int(idx_str))\n",
        "\n",
        "if not block_indices:\n",
        "    raise ValueError(\"No transformer blocks found in model!\")\n",
        "\n",
        "# Last transformer block\n",
        "last_block_idx = max(block_indices)\n",
        "\n",
        "# Unfreeze last block\n",
        "for name, param in ChatSPE.named_parameters():\n",
        "    if f\".blocks.{last_block_idx}.\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "# Unfreeze output layers\n",
        "for name, param in ChatSPE.named_parameters():\n",
        "    if name.startswith(\"model.output\") or name.startswith(\"model.mtp.output_head\"):\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "trainable_params = [p for p in ChatSPE.parameters() if p.requires_grad]\n",
        "total_trainable = sum(p.numel() for p in trainable_params)\n",
        "\n",
        "optimizer = torch.optim.AdamW(trainable_params, lr=5e-4, weight_decay=0.1)\n",
        "\n",
        "print(f\"Partial Fine-Tuning...\")\n",
        "print(f\"Training Last transformer block ({last_block_idx}) + output layer\")\n",
        "print(f\"Total trainable parameters: {total_trainable:,}\")\n",
        "\n",
        "# Partial Fine-tune\n",
        "train_losses, val_losses, _, _ = train_model(\n",
        "    ChatSPE,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    device=device,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=10,\n",
        "    print_every=2,\n",
        "    prompt=\"Prompt: What is porosity?\"\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0WT_1fIBfhq"
      },
      "source": [
        "# Step 12: Chat with your Model (Q&A)\n",
        "\n",
        "The below function `clean_response` extracts the model's response cleanly, removing the prompt and <|endoftext|> from the generated text."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_response(generated_text):\n",
        "    if not generated_text:\n",
        "        return \"Sorry, I couldn't generate a response.\"\n",
        "\n",
        "    text = str(generated_text)\n",
        "\n",
        "    # Print the prompt part\n",
        "    if \"Response:\" in text:\n",
        "        prompt_part = text.split(\"Response:\", 1)[0] + \"Response:\"\n",
        "    else:\n",
        "        prompt_part = \"\"\n",
        "\n",
        "    print(\"=======================================\")\n",
        "    print(f\"{prompt_part.strip()}\")\n",
        "\n",
        "    # Extract response\n",
        "    if \"Response:\" in text:\n",
        "        text = text.split(\"Response:\", 1)[1]\n",
        "\n",
        "    # Truncate at <|endoftext|>\n",
        "    if \"<|endoftext|>\" in text:\n",
        "        text = text.split(\"<|endoftext|>\", 1)[0]\n",
        "\n",
        "    # Remove non-printable characters\n",
        "    text = ''.join(c for c in text if c.isprintable() or c.isspace())\n",
        "\n",
        "    # If text is empty after cleaning, return a default message\n",
        "    if not text.strip():\n",
        "        return \"I'm not sure how to answer that. Could you ask in a different way?\"\n",
        "\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "XJkRGnZ0oM3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_upbhQAHA6p1"
      },
      "source": [
        "## Chat with **ChatSPE**\n",
        "\n",
        "Below is the new prompt for chatting with our model. Please check the format of `new_prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRIedBN7Fkf1",
        "outputId": "6f4e0987-57f2-4880-f49c-a701bd8a82f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================\n",
            "Prompt: What is water saturation?\n",
            "Response:\n",
            "Oil recovery factor is the fraction of a pressure pore volume of a fluids.\n",
            "The fraction of oil production data by water influx is the following steps:\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "new_prompt = \"Prompt: What is water saturation?\\nResponse:\" # replace it with your prompt\n",
        "\n",
        "generated_answer = generate_text(\n",
        "    ChatSPE,          # fine-tuned model\n",
        "    tokenizer,\n",
        "    new_prompt,\n",
        "    max_length=50,    # max tokens to generate\n",
        "    temperature=0.7,  # adjust creativity\n",
        "    top_k=10,         # top-k sampling\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "clean_answer = clean_response(generated_answer)\n",
        "print(clean_answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkC7duZSDkDQ"
      },
      "source": [
        "Our fine-tuned model's prediction is still not very accurate. After all, the pretrained model was trained on just two books, and the fine-tuning used only ten samples. Generally, the more data, the more accurate the model's predictions.\n",
        "\n",
        "**Evaluating the finetuned Model**\n",
        "\n",
        "If you want to automate the response evaluation of the finetuned LLM using another LLM, please check **Section 7.8 Evaluating the finetuned LLM** of Sebastian Raschka GitHub repo of Chapter 7: Finetuning To Follow Instructions. Link is [here](https://github.com/rasbt/LLMs-from-scratch/blob/c278745aff419ae6c1d6409ca4279aa57ea749e4/ch07/01_main-chapter-code/ch07.ipynb).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 13: Deploying Model to Production\n",
        "\n",
        "Once you are satisfied with your model's predictions, it's time to deploy it to production. There are multiple deployment options for commercial use. For commercial deployment, common options include AWS (SageMaker/EC2), Google Cloud (Vertex AI), and Azure ML. However, for educational or demo purposes, as in our case, popular free option is Hugging Face Spaces.\n",
        "\n",
        "### Deployment using Hugging Face Spaces\n",
        "\n",
        "1. Go to https://huggingface.co and log in or sign up.\n",
        "2. Navigate to Spaces → + New Space.\n",
        "3. Choose a space name (e.g., ChatSPE).\n",
        "4. Select:\n",
        "    * SDK: Gradio\n",
        "    * Hardware: CPU Basic\n",
        "    * Visibility: Public\n",
        "5. Click Create Space.\n",
        "\n",
        "Your space will be created with default files such as `.gitattributes` and `README.md`. You now need to upload the following four files (use Git, VS Code, or the web editor if direct upload is unavailable):\n",
        "\n",
        "<pre>\n",
        "ChatSPE\n",
        "├─ app.py\n",
        "└─ model.py\n",
        "└─ requirements.txt\n",
        "└─ chatspe.pt\n",
        "</pre>\n",
        "\n",
        "### File Overview\n",
        "* `app.py`: Contains the Gradio interface code required by Hugging Face. You can modify the title, description, and examples to match your project.\n",
        "* `model.py`: Includes the full model implementation discussed earlier (excluding training and data preparation).\n",
        "* `chatspe.pt`: Your fine-tuned model (see below code).\n",
        "* `requirements.txt`: Lists all Python dependencies.\n",
        "\n",
        "Once the build process completes successfully, your model will be live. You can interact with it in a ChatGPT-like interface and share the public link with others. The following space is the exact model trained and deployed in this guide:\n",
        "https://huggingface.co/spaces/saifkhanengr/ChatSPE"
      ],
      "metadata": {
        "id": "azvELEjwPSeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code freeze (disable training behavior) and save our fine-tuned model in the folder named ChatSPE."
      ],
      "metadata": {
        "id": "riRXivRviOva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save fine-tuned model\n",
        "\n",
        "ChatSPE.eval()\n",
        "for p in ChatSPE.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# Save model weights only in ChatSPE folder\n",
        "import os\n",
        "os.makedirs(\"./ChatSPE\", exist_ok=True)\n",
        "torch.save(ChatSPE.state_dict(), \"./ChatSPE/chatspe.pt\")\n",
        "print(f\"chatspe.pt saved in ChatSPE\")\n"
      ],
      "metadata": {
        "id": "Ol6jZbbJaFQT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b3f6cc9-33f0-4e92-fe1d-1e578588fb5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chatspe.pt saved in ChatSPE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There you have it. You now know how to train your own LLM from scratch and fine-tune any model. This is the end of our journey. I hope you have learned a thing or two. However, remember that there is *more to do than can ever be done.* Learning is a lifelong journey, so keep learning, questioning, and researching, because *the more we know, the more we know that the less we know.*\n",
        "\n",
        "If you notice any mistakes in my implementation or have suggestions for improvement, please feel free to open a GitHub issue or reach out to me on [LinkedIn](https://www.linkedin.com/in/saifkhanengr/) or [Facebook](https://www.facebook.com/saifkhanengr).\n",
        "\n",
        "Goodbye and see you in the next project..."
      ],
      "metadata": {
        "id": "3XrNj8gfaIkI"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}